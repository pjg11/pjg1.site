<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pjg1.site</title>
    <link>https://pjg1.site/index.html</link>
    <description>Recent posts on pjg1.site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://pjg1.site/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Being intentional about engaging with other&#39;s blogs</title>
      <link>https://pjg1.site/intentional-engagement.html</link>
      <pubDate>Mon, 10 Nov 2025 23:15:02 +0400</pubDate>
      
      <guid>https://pjg1.site/intentional-engagement.html</guid>
      <description><![CDATA[<p>An addendum in the article <a href="https://vhbelvadi.com/blogging-together">Looking beyond collective blogging</a>&mdash;a conversation between the author V.H.
Belvadi and <a href="https://hamatti.org">Juhis</a>&mdash;flipped a switch in my brain:</p>
<blockquote>
<p>Juhis rightly pointed out that ‘the feeling of connection comes through putting an effort into it rather than through automation.&rsquo; In other words most people write and expect engagement without putting effort into participating with others themselves. So, while writing is integral to blogging, we should start treating participation with other blogs as a similarly important activity.</p></blockquote>
<p>I think this quote might be one of the reasons why previous attempts at blogging regularly didn&rsquo;t stick. I have a tendency to look inwards when creating. The focus is always on &ldquo;where I could improve my writing&rdquo; or &ldquo;how I could make my work reach better&rdquo;.</p>
<p>Aside from working on my posts this time round, I would like to try being more intentional about engaging with other&rsquo;s work: reaching out to those who&rsquo;s posts resonate, either via email/social media or by sharing them there along with a bit of commentary.</p>
<p>The article also talks about about the <a href="https://indieweb.org">IndieWeb</a> and specifically <a href="https://indieweb.org/Webmention">Webmentions</a>, that I have been curious about. However, I want to commit to interacting manually first, before I choose to incorporate automation into my interactions.</p>
<p>via <a href="https://lars-christian.com/notes/2025-08-24-the-loneliness-of-blogging/">lars-christian.com</a></p>
]]></description>
    </item>
    
    <item>
      <title>Starting a Homelab</title>
      <link>https://pjg1.site/starting-a-homelab.html</link>
      <pubDate>Sat, 08 Nov 2025 23:59:00 +0400</pubDate>
      
      <guid>https://pjg1.site/starting-a-homelab.html</guid>
      <description><![CDATA[<p>I have a spare laptop&mdash;a <a href="https://support.apple.com/en-us/111947">15-inch MacBook Pro from 2017</a>&mdash;that I installed Ubuntu on sometime last year to understand Linux better. I did get a better understanding of how the hardware and OS work together (even <a href="/mbp-linux-wifi.html">wrote</a> <a href="/mbp-linux-power.html">posts</a> about it), but quickly ran into a roadblock on what else I could work on. My goals were to understand networking and develop Linux sysadmin skills.</p>
<p>I started lurking on <a href="https://www.reddit.com/r/homelab">r/homelab</a> probably around the same time. Something about the term &ldquo;lab&rdquo; brings so much joy to me&mdash;a space where one could tinker to their heart&rsquo;s content, break things and fix them, and then share their findings. However, along with joy also came feelings of overwhelm, as most of the posts I saw there went over my head.</p>
<p>It was after a few chats with people at the <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a> who were also working on their homelabs and searching for beginner posts on <a href="https://www.reddit.com/r/homelab">r/homelab</a> that made things click for me.</p>
<p>Instead of running things on a single operating system or, what I needed was a hypervisor. I used to setup Linux/Windows virtual machines on my daily driver, but often had to delete them as they would take up a lot of space. Having a dedicated machine turned into a hypervisor solves this problem. The machine I&rsquo;m using has 2TB of storage, which is more than enough to create and delete as many VMs as I want. I chose <a href="https://www.proxmox.com/en/products/proxmox-virtual-environment/overview">Proxmox</a> as the hypervisor, which is built on top of the Debian Linux distribution.</p>
<p>Using a laptop instead of an actual server meant that I had some constraints:</p>
<ol>
<li>
<p>The machine connects to the Internet via WiFi and does not have an Ethernet port. Proxmox doesn&rsquo;t recommend using WiFi, however I tried my best to make it work. When I realized that it is too much of a limitation, I purchased a USB-to-Ethernet adapter. I learnt about how to get an Ethernet connection from my home router to my room for the first time, which was fun!</p>
</li>
<li>
<p>The machine is a laptop with a battery in it and the battery is quite power hungry, so keeping the machine up all day while plugged in wasn&rsquo;t seeming like a good idea. So I decided that I wouldn&rsquo;t keep the machine up at all times, and use it like a regular laptop.</p>
</li>
</ol>
<p>Another constraint that I personally set was to rely on virtualization as much as possible and introduce additional hardware only where required. I wanted to get started quickly and not fall down the rabbit hole of deciding what hardware to buy. I also wanted to test the limits of the machine and tools I already had.</p>
<p>Here is a list of things I plan to work on, each of which might turn into one or more blog posts:</p>
<ul>
<li>
<p>Setting up a virtual router to create an internal virtual network. This would help me learn about routing, <abbr title="Virtual LAN">VLAN</abbr>s, network protocols such as <abbr title="Dynamic Host Configuration Protocol">DHCP</abbr>, and maybe even set up a <abbr title="Virtual Private Network">VPN</abbr> or a wireless access point to help me access the network from another machine.</p>
</li>
<li>
<p>An authoritative <abbr title="Domain Name Server">DNS</abbr> server for the virtual network. I&rsquo;ve been curious about what setting up a DNS server might look like, and it would be nice to not have to remember the IP address and use domain names instead.</p>
</li>
<li>
<p>A ethical hacking environment, which would include a Kali Linux and a Windows VM that I can use when participating in <abbr title="Capture the Flag">CTF</abbr>s. I&rsquo;ve also heard about people setting up VMs for learning Active Directory, emulating <abbr title="Security Operations Center">SOC</abbr>-like environments, but I haven&rsquo;t decided on details for it yet.</p>
</li>
<li>
<p>This might be against the &ldquo;not keeping the server up all the time&rdquo; constraint, but I have a lot of space on this machine + two external hard drives. A DIY <abbr title="Network Attached Storage">NAS</abbr> might be an interesting project to work on.</p>
</li>
</ul>
]]></description>
    </item>
    
    <item>
      <title>Switching from Jekyll to Hugo</title>
      <link>https://pjg1.site/jekyll-to-hugo.html</link>
      <pubDate>Fri, 07 Nov 2025 23:18:03 +0400</pubDate>
      
      <guid>https://pjg1.site/jekyll-to-hugo.html</guid>
      <description><![CDATA[<p>I switched from Jekyll to Hugo as my static site generator of choice recently. The only reason I have a Ruby environment is for this site, so moving from that to a single binary looked appealing.</p>
<p>I use a specific version of Hugo by downloading the binary directly instead of via a package manager. This was a tip I got from a <a href="https://jvns.ca/blog/2016/10/09/switching-to-hugo/">similar post</a> from Julia Evans. I liked the idea to be able to choose when to upgrade instead of my site breaking each time something changes with a new release.</p>
<p>Julia&rsquo;s post also mentioned a way to import most of my Jekyll site with one command: <code>hugo import jekyll</code>. It moved the posts static assets (CSS, images) almost seamlessly, except for a few changes in the configuration and specific posts to render certain parts correctly. However, it did not import any of my templates, so the site wouldn&rsquo;t render without adding a theme.</p>
<p>Themeing is one thing that has confused me each time I&rsquo;ve tried to switch to Hugo in the past since I prefer making custom layouts. Hugo also offers a few base layout files that Jekyll did not, which took a while to get the hang of. Here&rsquo;s the approach I took:</p>
<ul>
<li>I created a blank theme by running <code>hugo new theme</code> and looked up a post or two to understand the structure better.</li>
<li>Then I simplifed the theme files further to get as close to my Jekyll layouts as possible.</li>
<li>Then I moved the relevant directories (<code>archetypes</code> and <code>layouts</code>) from the theme folder to the root directory of the site, a structure that is familiar to me from Jekyll.</li>
<li>Lastly I deleted the theme folder and ensured that my site still renders correctly.</li>
</ul>
<p>The last set of changes I made were configuration changes, and the site was ready to deploy!</p>
<hr>
<p>On one hand, the amount of configuration options in Hugo can get overwhelming, and the templating language is also confusing coming from Jekyll&rsquo;s Liquid, making Hugo&rsquo;s learning curve steep.</p>
<p>Where I prefer Hugo to Jekyll is when structuring the site files. Jekyll imposes a certain filename format and directory for blog posts, and any other collections/sections have to be configured differently. I reached a roadblock whenever I wanted to create other collections or sub-collections within the post collection. Hugo doesn&rsquo;t impose such structure, and lets you define your own.</p>
<p>I&rsquo;m looking forward to tinkering with Hugo more.</p>
]]></description>
    </item>
    
    <item>
      <title>A custom GitHub Actions workflow for static HTML pages</title>
      <link>https://pjg1.site/gh-workflow-html.html</link>
      <pubDate>Thu, 06 Nov 2025 05:22:00 +0400</pubDate>
      
      <guid>https://pjg1.site/gh-workflow-html.html</guid>
      <description><![CDATA[<p>I build files for this website locally and push the static HTML files to GitHub Pages, using the default workflow. At some point, I started noticing that the deploys were taking long to complete. A deploy from today shows that it took 40 seconds to render.</p>
<p>I decided to looked into the workflow itself using <a href="https://cli.github.com">GitHub&rsquo;s command line tool</a>, and found a job called <code>build</code>, which takes the most time to complete.</p>
<pre><code>$ gh run list --limit 1
STATUS  TITLE              WORKFLOW        BRANCH  EVENT    ID           ELAPSED  AGE
✓       pages build an...  pages-build...  main    dynamic  19118060009  40s      about 50 mi...
$ gh run view 19118060009

✓ main pages-build-deployment · 19118060009
Triggered via dynamic about 50 minutes ago

JOBS
✓ build in 22s (ID 54632038648)
✓ deploy in 8s (ID 54632067818)
✓ report-build-status in 5s (ID 54632067849)
...
</code></pre>
<p>What does <code>build</code> do? It builds the site with <a href="https://jekyllrb.com">Jekyll</a>.</p>
<pre><code>$ gh run view --job=54632038648

✓ main pages-build-deployment · 19118060009
Triggered via dynamic about 50 minutes ago

✓ build in 22s (ID 54632038648)
  ✓ Set up job
  ✓ Pull ghcr.io/actions/jekyll-build-pages:v1.0.13
  ✓ Checkout
  ✓ Build with Jekyll
  ✓ Upload artifact
  ✓ Post Checkout
  ✓ Complete job
</code></pre>
<p>My website doesn&rsquo;t need to be built by Jekyll, and GitHub provides a <a href="https://github.blog/news-insights/the-library/bypassing-jekyll-on-github-pages/">solution</a> to skip it&mdash;to add a file called <code>.nojekyll</code> to the root of the repository.</p>
<p>So I added the file and pushed the changes, and the workflow ran again. This time it took 20 seconds to run, half the time from the last workflow! However, the build step is still there.</p>
<pre><code>$ gh run list --limit 1
STATUS  TITLE              WORKFLOW        BRANCH  EVENT    ID           ELAPSED  AGE
✓       pages build an...  pages-build...  main    dynamic  19118930798  20s      about 9 min...
$ gh run view 19118930798

✓ main pages-build-deployment · 19118930798
Triggered via dynamic about 9 minutes ago

JOBS
✓ build in 4s (ID 54634839639)
✓ report-build-status in 3s (ID 54634846786)
✓ deploy in 8s (ID 54634846814)
...
</code></pre>
<p>I was under the assumption that <code>.nojekyll</code> would remove the build job, but that wasn&rsquo;t the case. Instead, it only removes one of the steps in the job, that runs Jekyll.</p>
<pre><code>$ gh run view --job=54634839639

✓ main pages-build-deployment · 19118930798
Triggered via dynamic about 9 minutes ago

✓ build in 4s (ID 54634839639)
  ✓ Set up job
  ✓ Checkout
  ✓ Upload artifact
  ✓ Post Checkout
  ✓ Complete job
...
</code></pre>
<p>It feels unnecessary to have to run the <code>build</code> and <code>report-build-status</code> jobs each time, when all I really need is just the <code>deploy</code> job. I wanted to speed this up further.</p>
<p>After some clickity-clakity on the Pages section of the repository settings, I discovered two options for deployment. One is to deploy from a branch, which is the default GitHub Pages workflow. The other is GitHub Actions, which allows for custom workflows.</p>
<p>Clicking on the GitHub Actions option displayed some suggested templates. One of those templates is <a href="https://github.com/actions/starter-workflows/blob/main/pages/static.yml">Static HTML</a>, which sounds like exactly what I&rsquo;m looking for! Here is an excerpt:</p>
<pre><code>jobs:
  # Single deploy job since we're just deploying
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup Pages
        uses: actions/configure-pages@v5
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          # Upload entire repository
          path: '.'
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
</code></pre>
<p>In comparison to the GitHub Pages workflow which had three jobs, this has only one called <code>deploy</code>&mdash;perfect! I took the template as is and added it to my repository at <code>.github/workflows/</code>. One thing I changed was commenting out the <strong>Setup Pages</strong> step, as I didn&rsquo;t recall seeing that step in the previous workflows.</p>
<p>I expected a much bigger speedup for the custom workflow run, but it was only a second faster.</p>
<pre><code>$ gh run list --limit 1
✓       *                  Deploy stat...  main    push     19120221474  18s      about 15 mi...
$ gh run view 19120221474

✓ main Deploy static content to Pages · 19120221474
Triggered via push about 15 minutes ago

JOBS
✓ deploy in 14s (ID 54638891569)
...
</code></pre>
<p>Looking closely at the steps, I get a better sense why. The <code>deploy</code> job is now a mix of the <code>build</code> and <code>deploy</code> jobs from the second workflow run (after adding <code>.nojekyll</code>). The number of jobs may have reduced from three to one, but the number of steps are more or less the same.</p>
<p>Combining all steps into one job does reduce some overhead. The original workflow ran a setup and cleanup for each of the three jobs. With one job, the setup and cleanup happens only once, which saves a second or two.</p>
<p>I&rsquo;ll take a few seconds of speedup as a win.</p>
]]></description>
    </item>
    
    <item>
      <title>A minimal keyboard key effect with CSS</title>
      <link>https://pjg1.site/kbd-css.html</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/kbd-css.html</guid>
      <description><![CDATA[<p>I use the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/kbd"><code>kbd</code></a> element to specify keypresses in my posts. To differentiate it from the inline <code>code</code> element, I&rsquo;ve styled it as a minimal version of an actual key:</p>
<pre><code>kbd {
  font-family: ui-monospace, monospace;
  font-size: 90%;
  margin: 0 0.07rem;
  padding: 0.07rem 0.35rem;
  border: 0.07rem solid;
  border-bottom: 0.18rem solid;
  border-radius: 0.21rem;
}

kbd:hover {
  border-bottom: 0.07rem solid;
  vertical-align: -0.1rem;
  cursor: text;
}
</code></pre>
<p>The hover effect is inspired from <a href="https://dylanatsmith.com/wrote/styling-the-kbd-element">Styling the kbd element</a> by Dylan Smith. I experimented with the <code>border-bottom</code> and <code>vertical-align</code> values till I found a combination that recreated the hover effect well.</p>
<p>Here are keys from the QWERTY keyboard layout as an example:</p>
<div class="keyboard">
<kbd>Q</kbd><kbd>W</kbd><kbd>E</kbd><kbd>R</kbd><kbd>T</kbd><kbd>Y</kbd><kbd>U</kbd><kbd>I</kbd><kbd>O</kbd><kbd>P</kbd><br>
<kbd>A</kbd><kbd>S</kbd><kbd>D</kbd><kbd>F</kbd><kbd>G</kbd><kbd>H</kbd><kbd>J</kbd><kbd>K</kbd><kbd>L</kbd><br>
<kbd>Z</kbd><kbd>X</kbd><kbd>C</kbd><kbd>V</kbd><kbd>B</kbd><kbd>N</kbd><kbd>M</kbd>
</div>
<style>
div.keyboard {
  text-align: center;
  line-height: 1.7
}
kbd {
  font-family: ui-monospace, monospace;
  font-size: 90%;
  margin: 0 0.07rem;
  padding: 0.07rem 0.35rem;
  border: 0.07rem solid;
  border-bottom: 0.18rem solid;
  border-radius: 0.21rem;
}
kbd:hover {
  border-bottom: 0.07rem solid;
  vertical-align: -0.1rem;
  cursor: text;
}
</style>
]]></description>
    </item>
    
    <item>
      <title>Faded codeblocks using CSS</title>
      <link>https://pjg1.site/fade-block-css.html</link>
      <pubDate>Wed, 26 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/fade-block-css.html</guid>
      <description><![CDATA[<p>One aspect I&rsquo;ve gotten stuck on with the styling of this blog is code blocks. I&rsquo;ve tried adding a border and/or a background color in the past, but I couldn&rsquo;t stick with either of them and constantly kept changing styles.</p>
<p>I wanted a subtler indication to scroll if a block overflows, and a fading gradient towards seemed like a good option.</p>
<p>Here&rsquo;s the CSS I wrote for it:</p>
<pre><code>pre {
  position: relative;
  background: white;
}
pre::after {
  content: &quot;&quot;;
  position: absolute;
  top: 0;
  bottom: 0;
  left: 95%;
  right: 0;
  background-image: linear-gradient(to right, transparent, white);
}
@media (prefers-color-scheme: dark) {
  pre {
    background: black;
  }
  pre::after {
    background-image: linear-gradient(to right, transparent, black);
  }
}
pre code {
  display: block;
  padding: 0.75rem 0;
  overflow: auto;
  padding-inline-end: 1.5rem;
}
</code></pre>
<p>The key behind this effect is the <code>::after</code> pseudo-element, which is a linear-gradient positioned to the right end of the <code>pre</code> block. The <code>left</code> value ensures that the gradient doesn&rsquo;t overlap the block completely, and acts as a subtle gradient, suggesting the user to scroll to see the code.</p>
<p>Usually I&rsquo;d add the scroll to the <code>pre</code> block, however since we want the pseudoelement to stay at a fixed position, the scroll and overflow is applied to the child block - the <code>code</code> element in this case.</p>
<p>The gradient hiding overflow text makes sense, but this also covers the text at the end of the scroll. This is fixed by adding <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/padding-inline-end"><code>padding-inline-end</code></a> to the <code>code</code> element, which adds padding at the end of scroll.</p>
<p>Here&rsquo;s how it looks:</p>
<pre class="block"><code>This is a super long string of text that appears to be faded where the text overflows.</code></pre>
<style>
  pre.block {
    position: relative;
    background: #000;
    color: #fff;
    border: unset;
  }
  pre.block code {
    background: unset;
    border: unset
  }
  pre.block::after {
    content: "";
    position: absolute;
    top: 0;
    bottom: 0;
    left: 95%;
    right: 0;
    background-image: linear-gradient(to right, transparent, #000);
  }
  pre.block code {
    display: block;
    padding: 0.75rem 0;
    overflow: auto;
    padding-inline-end: 1.5rem;
  }
</style>
<p>While I&rsquo;ve demonstrated this for codeblocks, this styling could be extended for any block element - say a paragraph within a div, or a paragraph within a blockquote.</p>
]]></description>
    </item>
    
    <item>
      <title>Baby&#39;s first monitoring system</title>
      <link>https://pjg1.site/first-monitoring-system.html</link>
      <pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/first-monitoring-system.html</guid>
      <description><![CDATA[<p>Till last week, I didn&rsquo;t know what a monitoring system really looked like. A week later, I&rsquo;m in the process of setting up one for the <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a>&rsquo;s shared computing cluster<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  which is community maintained. Here are some notes about the various tools I&rsquo;m using and how they work together (<a href="#tldr">TL;DR</a>).</p>
<p>I&rsquo;m halfway through my second batch at RC, and one of my batch goals was to learn DevOps/SRE skills by contributing to this cluster. Having put it off for the first 5 weeks, I finally reached out to folks in the weekly meeting about the cluster, where I was recommended to look into <a href="https://prometheus.io">Prometheus</a>.</p>
<h2 id="prometheus">Prometheus</h2>
<p>The Prometheus server at its core is a database. More specifically, it is a time-series database, which means it stores key-value pairs with the key being a timestamp, thus showing how a particular value changed over time. This data can be used to create graphs and dashboards or trigger alerts if the values cross a certain threshold (more on both later). It has its own query language called PromQL.</p>
<p>The server can pull and store data from multiple machines, so it runs on only one of the machines in the cluster. However, if this machine goes down for some reason, our monitoring system is down.</p>
<h2 id="node-exporter">Node Exporter</h2>
<p>We have a database, cool, but where does the data come from? There are a variety of tools for this<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, but the one I&rsquo;m using here is Prometheus&rsquo; own tool - <a href="https://github.com/prometheus/node_exporter">Node Exporter</a> - that captures metrics from the system - things like CPU usage, memory usage, filesystem sizes, etc. This runs on each machine in the cluster.</p>
<h2 id="alertmanager">Alertmanager</h2>
<p>The last piece in the puzzle is alerting. The Prometheus server takes in alert rules written in PromQL - things like checking for low disk space, checking if certain services failed to run, high CPU or memory usage, etc.</p>
<p>It checks the metrics against the rules as they arrive. If a rule is met, it sends the alert to a tool called <a href="https://github.com/prometheus/alertmanager">Alertmanager</a> that handles sending of notifications via email or a chat platform. RC uses Zulip for communication, which has an <a href="https://zulip.com/integrations/doc/alertmanager">integration for Alertmanager</a> that I&rsquo;m using in this case.</p>
<h2 id="grafana">Grafana</h2>
<p>Grafana can be integrated with Prometheus to visualize the metrics via graphs and dashboards. I&rsquo;ve mainly been focused on getting alerting to work so far, so I am yet to try making a dashboard.</p>
<h2 id="adding-kubernetes-to-the-mix">Adding Kubernetes to the mix</h2>
<p>The reason I was recommended Prometheus in the first place is because it had been deployed within a Kubernetes cluster by a fellow Recurser.</p>
<p>The above setup would have worked just fine if I ran them as individual services directly on the machine. However, I went ahead with the Kubernetes option for two reasons:</p>
<ol>
<li>I preferred using something that was already deployed over re-inventing the wheel</li>
<li>I&rsquo;d been hearing a lot about Kubernetes, so this would finally be my introduction to the tool</li>
</ol>
<p>One advantage of using Kubernetes is that it makes multiple machines operate as one big unit - you provide it a list of services to deploy, and it&rsquo;ll figure out which machine&rsquo;s resources to utilize and how. Except for Node Exporter which is deployed on all machines, other services like Grafana, Alertmanager and the Prometheus server are deployed automagically.</p>
<p>In my case they&rsquo;re deployed using Kubernetes&rsquo; package manager, Helm. While these eases setup, it also adds some layers of complexity.</p>
<p>Accessing the Prometheus web interface locally now requires more steps than a direct install, as it is isolated from the main system and has its own network and IP address. So multiple port forwards would be required.</p>
<p>Making changes to configuration files is also harder. Kubernetes containers don&rsquo;t have persistent disk space, so it isn&rsquo;t possible to exec into the containers and change files directly<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> like I would with a direct install. So I have to add the configuration to some external file and then pass that file during deployment.</p>
<p>For applications deployed using Helm, I have to modify something called a Helm chart. I don&rsquo;t completely understand what the various files are for, but one of them is <a href="https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml"><code>values.yaml</code></a>, where I would add custom configuration like the alert rules for example. This file is passed to the install command, which then applies the custom config.</p>
<p>This complexity was preventing me from testing stuff quickly, so I decided to break the project into two phases. The first phase was testing Prometheus and alerting using a direct install, which I&rsquo;m almost done with. Once I have a working set of config files, the next phase would be to figure out the Helm install and adding my configuration to the values file.</p>
<h2 id="tldr">TL;DR</h2>
<p>The following diagram is based on my limited understanding of the above concepts.</p>
<pre><code>┌───────────────┐   ┌──────────────────────────────────┐
│ node 1        │   │ kubernetes cluster               │
│               │   │                                  │
├───────────────┤   │                 ┌──────────────┐ │
│ node exporter │◀──┼──────┐          │ prometheus   │ │
└───────────────┘   │      │          │ server       │ │
┌───────────────┐   │      │  pull    │              │ │
│ node 2        │   │      │ metrics  │              │ │
│               │   │      ├──────────│              │ │
├───────────────┤   │      │          │              │ │
│ node exporter │◀──┼──────┤          │              │ │
└───────────────┘   │      │          ├──────────────┤ │
┌───────────────┐   │      │          │ alert rules  │ │
│ node 3        │   │      │          └──────────────┘ │
│               │   │      │                  │        │
├───────────────┤   │      │      rule  ┌─────┘        │
│ node exporter │◀──┼──────┤      met   │              │
└───────────────┘   │      │            ▼              │
┌───────────────┐   │      │    ┌──────────────┐       │      ┌─────────┐
│ node 4        │   │      │    │              │  fire alert  │         │
│               │   │      │    │ alertmanager │───────┼─────▶│  zulip  │
├───────────────┤   │      │    │              │       │      │         │
│ node exporter │◀──┼──────┘    └──────────────┘       │      └─────────┘
└───────────────┘   └──────────────────────────────────┘
</code></pre>
<p>Is baby&rsquo;s first monitoring system a bit complex? Yes.</p>
<p>Is baby learning new things and reaching <a href="https://www.recurse.com/self-directives#work-at-the-edge">the edge of their abilities</a> thanks to the complexity? ALSO YES!</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>I knew about the cluster in my first batch, but the thought of contributing to it came to mind only a year later, thanks to some folks from a later batch starting a meeting to discuss stuff relating to the cluster.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>I found that metrics were already being captured on the machines using another tool while working on this. That tool didn&rsquo;t provide any support for alerts though, so I chose to switch to using Prometheus.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Prometheus has a web interface, so I expected that I would be able to change configuration and alert rules from the interface directly. I didn&rsquo;t find a way to do so though, and had to edit the file and restart the service each time.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></description>
    </item>
    
    <item>
      <title>How does a Linux machine connect to the internet, really?</title>
      <link>https://pjg1.site/linux-internet-from-scratch.html</link>
      <pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/linux-internet-from-scratch.html</guid>
      <description><![CDATA[<p>Recently, I was brainstorming networking project ideas, I got curious on what goes behind connecting to the internet, and if I could do it from scratch.</p>
<p>I&rsquo;m delighted to report that the experiment was successful, and I thought of sharing it here! I&rsquo;ve tested this on Ubuntu, but I think it should work on any Linux distribution. If not, <a href="mailto:piya@pjg1.site">let me know</a>.</p>
<ul>
<li><a href="#identify-and-disable-existing-configuration">Identify and disable existing configuration</a></li>
<li><a href="#additional-setup-for-wireless-interfaces">Additional setup for wireless interfaces</a></li>
<li><a href="#setup-network-interface">Setup network interface</a></li>
<li><a href="#set-a-default-gateway">Set a default gateway</a></li>
<li><a href="#setup-dns">Setup DNS</a></li>
<li><a href="#bonus-dynamic-addresses-via-dhcp">Bonus: Dynamic addresses via DHCP</a></li>
</ul>
<h2 id="identify-and-disable-existing-configuration">Identify and disable existing configuration</h2>
<p>Before I could set stuff up manually, I had to figure out my machine&rsquo;s existing configuration and disable it, so it wouldn&rsquo;t interfere with my handcrafted setup.</p>
<p>The <a href="https://documentation.ubuntu.com/server/explanation/networking/configuring-networks/">Ubuntu documentation</a> was a useful resource to find out the services in use. The network on my machine is configured using NetworkManager and DNS is managed using the systemd-resolved service.</p>
<p>I figured out what the above tools had setup using by trying out some of the code snippets in the docs, so I had a plan and a final result in mind.</p>
<p>Based on this, I made a note of the following from the existing configuration, which can be found by running <code>ip addr show</code>:</p>
<ul>
<li>Interface name - typically starts with one of <code>eth</code>, <code>en</code>, <code>wlan</code> or <code>wl</code>.</li>
<li>IP address associated with the interface</li>
<li>Subnet mask - the slash next to the IP address</li>
</ul>
<p>Once I had the information noted down, I disabled<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> NetworkManager and systemd-resolved (both running as <code>systemd</code> services) and set the network interface to down:</p>
<pre><code># systemctl stop NetworkManager
# systemctl disable NetworkManager
Removed &quot;/etc/systemd/system/network-online.target.wants/NetworkManager-wait-online.service&quot;.
Removed &quot;/etc/systemd/system/multi-user.target.wants/NetworkManager.service&quot;.
Removed &quot;/etc/systemd/system/dbus-org.freedesktop.nm-dispatcher.service&quot;.
# systemctl stop systemd-resolved
# systemctl disable systemd-resolved
# ip link set dev wlp3s0 down
</code></pre>
<p>With this, the machine is no longer connected to the internet.</p>
<h2 id="additional-setup-for-wireless-interfaces">Additional setup for wireless interfaces</h2>
<p>There are two types of interfaces you could be setting up.</p>
<p>One is for a connection made by connecting an Ethernet cable to your machine. If you were to try out this post on a Linux VM, you would be setting up an Ethernet connection and can skip this section. The other is a wireless interface, which can connect to WiFi networks.</p>
<p>An Ethernet interface appears up/enabled at all times - even before it has actual internet access - as its connected via cable. Wireless interfaces on the other hand remain down/disabled until you connect to a WiFi network.</p>
<p>This led to differences during setup, which required me to add separate instructions for both, making the post long and confusing.</p>
<p>It is possible to connect to a WiFi network before having internet access - this would be similar to situations when your phone or laptop displays a &ldquo;No Internet Connection&rdquo; message while being connected to a network.</p>
<p>The tool that helps connect to WiFi is <a href="https://www.linuxfromscratch.org/blfs/view/stable-systemd/basicnet/wpa_supplicant.html"><code>wpa_supplicant</code></a>. This is what the previous setup used, so I went with it. There may be a process for it running in the background from the previous setup which is no longer required, so you can terminate it if it exists:</p>
<pre><code># ps -ef | grep -i [w]pa
root         883       1  0 01:59 ?        00:00:00 /usr/sbin/wpa_supplicant -u -s -O DIR=/run/wpa_supplicant GROUP=netdev
# systemctl stop wpa_supplicant
# systemctl disable wpa_supplicant
</code></pre>
<p>The tool takes a configuration file, <code>wpa_supplicant.conf</code>, which contains information about the WiFi network you wish to connect to.</p>
<pre><code># cat &lt;&lt;EOF &gt; /etc/wpa_supplicant/wpa_supplicant.conf
ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
update_config=1

network={
    ssid=&quot;&lt;name&gt;&quot;
    psk=&quot;&lt;password&gt;&quot;
}
EOF
</code></pre>
<p>Replace <code>&lt;name&gt;</code> and <code>&lt;password&gt;</code> with your WiFi&rsquo;s name and password in plaintext. Yes, you read that right - a PASSWORD stored in PLAINTEXT<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. I&rsquo;m pretty shocked by this, but it seems to be a norm for WiFi tools, not sure why<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>I was following <a href="https://ubuntuforums.org/showthread.php?t=571188">this tutorial</a> which added further details to the network block like the protocol type and the encryption used. However, adding just the username and password seemed to work in my case.</p>
<p>Then, I ran <code>wpa_supplicant</code> with the config file:</p>
<pre><code># wpa_supplicant -D nl80211 -i wlp3s0 -c /etc/wpa_supplicant/wpa_supplicant.conf -B
Successfully initialized wpa_supplicant
</code></pre>
<p>This is run as a background process (<code>-B</code>) so I can continue using the terminal to type other commands. I can confirm if the connection took place successfully via <code>iw</code>:</p>
<pre><code># iw dev wlp3s0 info
Interface wlp3s0
    ifindex 2
    wdev 0x1
    addr &lt;MAC&gt;
    ssid &lt;name&gt;
    type managed
    wiphy 0
...
</code></pre>
<p>If the name next to the <code>ssid</code> field matches with name set in the configuration, that means the connection was successful.</p>
<h2 id="setup-network-interface">Setup network interface</h2>
<p>Without internet access, my network interface looked like this:</p>
<pre><code># ip addr show dev wlp3s0
2: wlp3s0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000
    link/ether &lt;MAC&gt; brd ff:ff:ff:ff:ff:ff
</code></pre>
<p>It needed an IP address to be able to talk to other machines that was missing. I assigned it one based on the information I noted down from the previous setup:</p>
<pre><code># ip addr add 192.168.100.128/24 dev wlp3s0
</code></pre>
<p><code>192.168.100.128</code> is the IP address and <code>/24</code> is the subnet. The subnet, - a shorthand for <code>255.255.255.0</code> - means that this network assigns addresses in the range of <code>192.168.100.X</code>, where <code>X</code> can be anywhere between 1 and 254 (0 and 255 are reserved).</p>
<p>I checked the interface after setting it to up, after which I can see the address!</p>
<pre><code># ip link set dev wlp3s0 up
# ip addr show dev wlp3s0
2: wlp3s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether &lt;MAC&gt; brd ff:ff:ff:ff:ff:ff
    inet 192.168.100.128/24 scope global wlp3s0
       valid_lft forever preferred_lft forever
</code></pre>
<p>With this, I was online!</p>
<p>Well, sort of. If I tried to ping another machine in the same network, it worked!</p>
<pre><code># ping -c3 192.168.100.141
PING 192.168.100.141 (192.168.100.141) 56(84) bytes of data.
64 bytes from 192.168.100.141: icmp_seq=1 ttl=64 time=4.09 ms
64 bytes from 192.168.100.141: icmp_seq=2 ttl=64 time=92.1 ms
64 bytes from 192.168.100.141: icmp_seq=3 ttl=64 time=113 ms

--- 192.168.100.141 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 4.088/69.678/112.835/47.144 ms
</code></pre>
<p>However, pinging a machine outside the network didn&rsquo;t.</p>
<pre><code># ping 1.1.1.1
ping: connect: Network is unreachable
</code></pre>
<p>There had to be a way to route packets outside of the network.</p>
<h2 id="set-a-default-gateway">Set a default gateway</h2>
<p>Accessing machines outside of the network requires a default gateway - an address that forwards packets to other networks when the destination address isn&rsquo;t part of the network&rsquo;s address range. In a home network, this address would likely be assigned to your router.</p>
<p>This information is added to the routing table, and is typically the first assignable address in the address range, <code>192.168.100.1</code> in this case. The default gateway was set using <code>ip route</code>:</p>
<pre><code># ip route add default via 192.168.100.1 dev wlp3s0
# ip route show
default via 192.168.100.1 dev wlp3s0
192.168.100.0/24 wlp3s0 proto kernel scope link src 192.168.100.128
</code></pre>
<p><code>ip route show</code> displays the routing table. The first rule is the one I just set, and the second one specifies routing for the entire address range, which was set after I assigned the address in the previous step.</p>
<p>Pinging to addresses outside of the network now worked!</p>
<pre><code># ping -c3 1.1.1.1
PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data.
64 bytes from 1.1.1.1: icmp_seq=1 ttl=59 time=23.4 ms
64 bytes from 1.1.1.1: icmp_seq=2 ttl=59 time=8.74 ms
64 bytes from 1.1.1.1: icmp_seq=3 ttl=59 time=7.11 ms

--- 1.1.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 7.113/13.093/23.426/7.336 ms
</code></pre>
<p>But if I were to try pinging a domain name, that wouldn&rsquo;t work.</p>
<pre><code># ping example.com
ping: example.com: Temporary failure in name resolution
</code></pre>
<p>So close, yet so far. The error message meant that it is unable to translate example.com to an IP address, which points towards a DNS issue.</p>
<h2 id="setup-dns">Setup DNS</h2>
<p>The process of translating domain names to IP addresses is done by a nameserver. These name servers are defined in <code>/etc/resolv.conf</code>, which on my machine was a symbolic link:</p>
<pre><code># ls -l /etc/resolv.conf
lrwxrwxrwx 1 root root 39 Feb  7 04:49 /etc/resolv.conf -&gt; ../run/systemd/resolve/stub-resolv.conf
</code></pre>
<p>This was part of the previous configuration, as DNS was setup using systemd-resolved on this machine. Since I&rsquo;ve disabled that, I removed the symlink and added my nameservers of choice. I used <a href="https://www.cloudflare.com/learning/dns/what-is-1.1.1.1/">Cloudflare&rsquo;s public DNS server</a> in this case:</p>
<pre><code># rm /etc/resolv.conf
# cat &lt;&lt;EOF &gt; /etc/resolv.conf
nameserver 1.1.1.1
nameserver 1.0.0.1
EOF
</code></pre>
<p>Pinging domain names finally worked!</p>
<pre><code># ping -c 1 example.com
PING example.com (96.7.128.198) 56(84) bytes of data.
64 bytes from a96-7-128-198.deploy.static.akamaitechnologies.com (96.7.128.198): icmp_seq=1 ttl=51 time=269 ms

--- example.com ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 269.178/269.178/269.178/0.000 ms
</code></pre>
<p>This brings mission &ldquo;connect to the Internet from scratch&rdquo; to an end! I had a lot of fun working on this and learnt a lot, I hope you enjoyed reading this too! Before I end the post though, there&rsquo;s one little side quest I wanted to cover.</p>
<h2 id="bonus-dynamic-addresses-via-dhcp">Bonus: Dynamic addresses via DHCP</h2>
<p>The IP address I set above is a static IP, which doesn&rsquo;t change. Each time I connect to the network, I can assign it the same address.</p>
<p>There are two problems with this:</p>
<ul>
<li>I need to know the address range for each network before I connect to it, which is time-consuming.</li>
<li>Setting an IP this way might cause confusion if another machine has been assigned the same address.</li>
</ul>
<p>The solution for this is to let the network assign an address when you connect to it, which is how your default setup most likely works. This is done using DHCP or the Dynamic Host Configuration Protocol.</p>
<p>I got it working using a tool called <code>dhclient</code>. It doesn&rsquo;t work if an IP address is already assigned to the interface, so I removed the static IP and default gateway I had set first:</p>
<pre><code># ip addr flush dev wlp3s0
# ip route flush dev wlp3s0
# dhclient -v wlp3s0
Internet Systems Consortium DHCP Client 4.4.3-P1
Copyright 2004-2022 Internet Systems Consortium.
All rights reserved.
For info, please visit https://www.isc.org/software/dhcp/

Listening on LPF/wlp3s0/&lt;MAC&gt;
Sending on   LPF/wlp3s0/&lt;MAC&gt;
Sending on   Socket/fallback
xid: warning: no netdev with useable HWADDR found for seed's uniqueness enforcement
xid: rand init seed (0x67d6369b) built using gethostid
DHCPDISCOVER on wlp3s0 to 255.255.255.255 port 67 interval 3 (xid=0xf29dbc1c)
DHCPOFFER of 192.168.100.128 from 192.168.100.1
DHCPREQUEST for 192.168.100.128 on wlp3s0 to 255.255.255.255 port 67 (xid=0x1cbc9df2)
DHCPACK of 192.168.100.128 from 192.168.100.1 (xid=0xf29dbc1c)
bound to 192.168.100.128 -- renewal in 271244 seconds.
</code></pre>
<p>From the output, it looks like the network&rsquo;s router (<code>192.168.100.1</code>) assigned this machine with the address <code>192.168.100.128</code>, which is what I was setting statically too.</p>
<p>What I also noticed was that running this also setup the default gateway and DNS automagically - and that too to the same address?!?!?</p>
<pre><code># ip route show | awk '/default via/{print $3}'
192.168.100.1
# cat /etc/resolv.conf
192.168.100.1
</code></pre>
<p>After some searching, I <a href="https://lobste.rs/s/563zjp/how_does_linux_machine_connect_internet">found</a> that my router (aka the default gateway) is also capable of handling DNS requests. More specifically, it can forward DNS requests to servers it has configured that are likely specified by my ISP, and then send it back to my machine. Pretty cool!</p>
<p>What&rsquo;s not so cool though, is that this one command basically automated almost everything I set up lovingly by hand :/ The experiment was still worth it though, as I now know exactly what steps the tool is automating.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>In my first attempt, I removed NetworkManager from the system all together, but reached a dead end and had to reinstall it. That&rsquo;s why I recommend disabling instead, as its easy to start over by enabling the service. &#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>It is possible to generate a password hash with a tool called <code>wpa_passphrase</code>, but turns out that you can use the hash as is to connect to a network without knowing the actual password. This kind of makes hashing pointless. &#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Even NetworkManager had my WiFi password stored in plaintext in a config file, which was a shocker. The rationale provided is that the file permissions are set such that only root can access it, making it safe. I&rsquo;m not so sure about that. &#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></description>
    </item>
    
    <item>
      <title>Things I learnt while working on ZulipFS</title>
      <link>https://pjg1.site/zulipfs.html</link>
      <pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/zulipfs.html</guid>
      <description><![CDATA[<p>I came across <a href="https://en.wikipedia.org/wiki/Filesystem_in_Userspace">FUSE</a> recently, which brought up an idea: <em>what if I could access a <a href="https://zulip.com">Zulip</a> instance as a filesystem?</em></p>
<p>This led to the creation of ZulipFS, where channels are represented as directories, and topics as files within those directories. The usage and code for the project is available <a href="https://github.com/pjg11/zulipfs">here</a>. This post talks about things I learnt about FUSE, filesystems in general and design choices I made in the process.</p>
<h2 id="how-fuse-works-in-a-nutshell">How FUSE works in a nutshell</h2>
<p>FUSE lets you mount a folder containing files and folders. These could be actual files and folders (eg: <a href="https://github.com/libfuse/sshfs">SSHFS</a> where files from a remote network are mounted) or virtual files (eg: <a href="https://omar.website/tabfs/">TabFS</a>, where information from browser tabs are presented as files and folders). You write your own implementation for relevant <a href="https://libfuse.github.io/doxygen/structfuse__operations.html">system calls</a> relating to file operations, and FUSE will run your implementation instead of the standard implementation.</p>
<p>The system calls I implemented in ZulipFS are:</p>
<ul>
<li><code>read</code> + <code>write</code> for reading and writing files</li>
<li><code>readdir</code> to list directory information</li>
<li><code>getattr</code> that returns the metadata for each file and directory</li>
</ul>
<h2 id="file-metadata-is-more-important-than-i-thought">File metadata is more important than I thought</h2>
<p>When I implemented and tested reading a message from a topic, only part of a message got printed. I checked if the API call was returning a partial message, but that seemed fine.</p>
<p>Turns out, I had the file size set to 512 bytes as I was working off of <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/useful/dnsfs.py">this example code</a>. So <code>read</code> checks for the size of a file and prints only that many bytes, which makes sense!</p>
<p>I now had to figure out a way to get the file size before the message has been read.</p>
<h2 id="reading-and-knowing-a-file-are-two-different-things">Reading and knowing a file are two different things</h2>
<p>The file size is set in the <code>getattr</code> function, which is called each time a file is read or listed. So the Zulip API would be called twice for a file read - once in <code>getattr</code> to get the length of the message, and then in <code>read</code> to display the message itself.</p>
<p>For this, I created a function called <code>get_topic</code> which can be called by <code>getattr</code> and <code>read</code>:</p>
<pre><code>def get_topic(self, channel, topic):
  channel_id = self.channels[channel]['stream_id']

  # returns the ID of the last message along with the topic name
  topicslist = self.client.get_stream_topics(channel_id)['topics']
  self.topics[channel] = { self.normalize(t['name']): t for t in topicslist }

  # get the message contents using the message ID
  message = self.client.get_raw_message(self.topics[channel][topic]['max_id'])
  message_fmt = f&quot;&quot;&quot;[{datetime.fromtimestamp(message['message']['timestamp'])}] {message['message']['sender_full_name']}
{message['raw_content']}
&quot;&quot;&quot;.encode()

  self.topics[channel][topic] = {
    'last_message': message_fmt,
    'last_timestamp': float(message['message']['timestamp']),
  }
  return self.topics[channel][topic]
</code></pre>
<p>Making the same API calls twice felt a bit excessive for reading, but it was okay as long as it wasn&rsquo;t slowing things down. Then I tried listing all the topics in a channel via <code>ls</code>, and things slowed down…A LOT.</p>
<p>Why, you ask? The function handling directory listing, <code>readdir</code>, calls <code>getattr</code> for EACH FILE in the directory. If a channel has 300 topics, that&rsquo;s 300 API calls before <code>ls</code> completes execution. To add to the chaos, <code>get_topic</code> above uses API calls instead of one, which means 600 API calls before <code>ls</code> completes execution. I had to find ways to optimize this.</p>
<h2 id="lazy-loading-files">Lazy loading files?!?!?</h2>
<p>The first optimization attempt was to remove the <code>get_topic</code> call from <code>getattr</code>, and call it only in <code>read</code>. I placed an exception block in <code>getattr</code>, which would assign a file size of 65535 bytes on mount, and a subsequent <code>read</code> would fill the hash map with the correct values, which <code>getattr</code> would take the next time its called.</p>
<pre><code>def getattr(self, path):
# ...snip...
# topic/file
try:
  channel, topic = path[1:].split('/')
  try:
    timestamp = self.topics[channel][topic]['last_timestamp']
  except KeyError:
    timestamp = now

  try:
    st.st_size = len(self.topics[channel][topic]['last_message'])
  except KeyError:
    st.st_size = 65535
# ...snip...
</code></pre>
<p>This worked initially, but caused problems when I wanted to append new messages instead of just displaying the last one. After lots of trial and error, a question popped up in my head: <em>What if I don&rsquo;t create all topic files right away, and add them only after someone tries to read or list it?</em></p>
<p>This seemed like a great idea as it would significantly reduce the number of API calls made at once. Things might slow down eventually as you read more and more topics, but it would still be faster than trying to list all topics at once.</p>
<p>Another optimization I was able to make was combining the two API calls into one using <a href="https://zulip.com/api/get-messages"><code>get_messages</code></a>, which powers Zulip&rsquo;s search functionality. I can pass it the name of a channel and topic, and ask it to return the last message of that topic. If either the channel or topic doesn&rsquo;t exist, it&rsquo;ll return an empty result.</p>
<pre><code>def get_topic(self, channel, topic):
  request = {
    &quot;anchor&quot;: &quot;newest&quot;,
    &quot;num_before&quot;: 1,
    &quot;num_after&quot;: 0,
    &quot;narrow&quot;: [
      {&quot;operator&quot;: &quot;channel&quot;, &quot;operand&quot;: self.channels[channel]['name']},
      {&quot;operator&quot;: &quot;topic&quot;, &quot;operand&quot;: self.zulip_name(topic)},
    ],
    &quot;apply_markdown&quot;: False,
  }
  try:
    message = self.client.get_messages(request)['messages'][0]
    message_fmt = f&quot;&quot;&quot;[{datetime.fromtimestamp(message['timestamp'])}] {message['sender_full_name']}
{message['content']}
&quot;&quot;&quot;.encode()

    self.topics[channel][topic] = {
      'last_message': message_fmt,
      'last_timestamp': float(message['timestamp']),
    }
  except IndexError:
    # channel or topic doesn't exist
    pass

  # if a channel or topic doesn't exist, this statement will cause an
  # exception in the function where this is called.
  return self.topics[channel][topic]
</code></pre>
<p>These optimizations made things fast enough that I could call <code>get_topic</code> from <code>getattr</code> again, so I could get rid of the extra try/except blocks:</p>
<pre><code># topic/file
try:
  channel, topic = path[1:].split('/')
  t = self.get_topic(channel, topic)
  st.st_mode = stat.S_IFREG | 0o644
  st.st_nlink = 1
  st.st_size = len(t['last_message'])
  st.st_mtime = t['last_timestamp']
except (KeyError, ValueError):
  return -errno.ENOENT
</code></pre>
<h2 id="appending-new-messages">Appending new messages</h2>
<p>I presented the pre-optimization version at the weekly <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a> presentations, and fellow batchmates <a href="https://eieio.games">Nolen</a> and <a href="https://bsky.app/profile/ohsh.it">Kevin O</a> suggested to add the ability to read new messages from a topic as they arrive by running <code>tail -f</code> on the file. This seemed like a good idea, and more useful than displaying just the last message.</p>
<p>I initially thought appending would require implementing a system call, but it was easier than I thought - if the timestamp of the current message is newer than the previous one, I append the new message to the end of the previous one in <code>get_topic</code>. I also needed an additional check for whether the topic had been read before or not, to initialize the file for the first time.</p>
<p>The <code>try</code> block in <code>get_topic</code> now looks like this:</p>
<pre><code>try:
  message = self.client.get_messages(request)['messages'][0]
  timestamp = float(message['timestamp'])
  message_fmt = f&quot;&quot;&quot;[{datetime.fromtimestamp(message['timestamp'])}] {message['sender_full_name']}
{message['content']}
&quot;&quot;&quot;.encode()

  if topic not in self.topics[channel]:
    # First message in file
    self.topics[channel][topic] = {
      'last_message': message_fmt,
      'last_timestamp': timestamp,
    }
    else:
      # Subsequent messages appended to file
      if timestamp &gt; self.topics[channel][topic]['last_timestamp']:
        self.topics[channel][topic] = {
          'last_message': self.topics[channel][topic]['last_message'] + b&quot;\n&quot; + message_fmt,
          'last_timestamp': timestamp,
        }
except IndexError:
  # channel or topic doesn't exist
  pass
</code></pre>
<h2 id="filename-gotchas">Filename gotchas</h2>
<p>One of the earliest errors I encountered was displaying names that had slashes in them. In Linux and other Unix-based OS&rsquo;s, a slash is considered as a delimiter for a directory rather than part of a filename. One thing I&rsquo;d seen certain apps do is change special characters to their URL-encoded versions, so I replaced all instances of <code>/</code> with <code>%2F</code>.</p>
<p>Another set of characters that are inconvenient to type in the terminal are emojis. I was initially thinking of getting rid of them, but then I realized that looking up the channel name would become tricky.</p>
<p>I remembered seeing textual representations of emojis, which turns out are called <a href="https://emojipedia.org/shortcodes">shortcodes</a>, and they&rsquo;re written as text in-between colons <code>:</code>. For example, the shortcode for 📝 is <code>:memo:</code>​, and these are understood by Zulip. Python has an <code>emoji</code> package that converts emojis to shortcodes and vice versa.</p>
<p>With that, I had two functions to convert Zulip names to a valid filename and vice versa.</p>
<pre><code>def file_name(self, name):
  return emoji.demojize(name.replace('/', '%2F'))

def zulip_name(self, name):
  return emoji.emojize(name.replace('%2F', '/'))
</code></pre>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to <a href="https://fractalkitty.com">Sophia</a> for reviewing a draft of this post.</p>
]]></description>
    </item>
    
    <item>
      <title>Making sense of zsh history shell options</title>
      <link>https://pjg1.site/zsh-history-opts.html</link>
      <pubDate>Thu, 09 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/zsh-history-opts.html</guid>
      <description><![CDATA[<p>I was refactoring my <code>.zshrc</code> recently, and found these existing options for managing the command history:</p>
<pre><code>setopt INC_APPEND_HISTORY
setopt SHARE_HISTORY
setopt HIST_IGNORE_DUPS
setopt HIST_IGNORE_ALL_DUPS
setopt HIST_SAVE_NO_DUPS
setopt HIST_IGNORE_SPACE
</code></pre>
<p>I think I copied these from somewhere whenever I last modified the file. But this time, I wanted to apply a more systematic approach to setting options, where I was sure I knew exactly what each option in the file was doing.</p>
<p>I stared at these options for a bit. Some of these look similar to each other, and I don&rsquo;t know how each one is different despite comments. Do I even need all of these?</p>
<p>For the purposes of this article, I&rsquo;ll be dividing the options into categories:</p>
<ul>
<li>Appending history - <code>SHARE_HISTORY</code>, <code>INC_APPEND_HISTORY</code></li>
<li>Managing duplicates - <code>HIST_IGNORE_DUPS</code>, <code>HIST_IGNORE_ALL_DUPS</code>, <code>HIST_SAVE_NO_DUPS</code></li>
<li><code>HIST_IGNORE_SPACE</code> makes sense right away, so I&rsquo;ll add it to my config directly.</li>
</ul>
<h2 id="appending-history">Appending history</h2>
<p>The default behavior for writing to the history file is to write all commands from a session in bulk at the end of the session. I&rsquo;m looking for a way to append commands from different sessions as they&rsquo;re entered to the history file, aka sharing the same file across sessions.</p>
<p>The solution for this was simple, <code>man zshoptions</code>:</p>
<pre><code>APPEND_HISTORY &lt;D&gt;
   If this is set, zsh sessions will append their history list to the
   history file, rather than replace it. Thus, multiple parallel zsh
   sessions will all have the new entries from their history lists
   added to the history file, in the order that they exit. [...]

INC_APPEND_HISTORY
   This option works like APPEND_HISTORY except that new history lines
   are added to the $HISTFILE incrementally (as soon as they are
   entered), rather than waiting until the shell exits.

SHARE_HISTORY &lt;K&gt;
   This option both imports new commands from the history file, and
   also causes your typed commands to be appended to the history file
   (the latter is like specifying INC_APPEND_HISTORY, which should be
   turned off if this option is in effect).
</code></pre>
<p>One of the first things that stood out was the fact that only one of these options needs to be set. <code>INC_APPEND_HISTORY</code> has the functionality of <code>APPEND_HISTORY</code> and part of <code>SHARE_HISTORY</code> works like <code>INC_APPEND_HISTORY</code>.</p>
<p><code>APPEND_HISTORY</code> didn&rsquo;t do what I wanted, so it was up to me to decide of making a choice between the latter two.</p>
<p>I&rsquo;m primarily concered around writing commands to the file, so that they&rsquo;re available in any sessions I start after it, not so much existing shell sessions (which <code>SHARE_HISTORY</code> does), so I chose <code>INC_APPEND_HISTORY</code>.</p>
<p>Another option I found in the <code>man</code> page was <code>INC_APPEND_HISTORY_TIME</code>, which works like <code>INC_APPEND_HISTORY</code> but appends the commands to the file once they&rsquo;ve completed, which I thought was cool.</p>
<h2 id="managing-duplicates">Managing duplicates</h2>
<p>The default behavior is to keep duplicates. I&rsquo;m looking for a way to store only the most recent version of a command and delete all instances of it from the file, as the command is entered.</p>
<p>Starting with the <code>man</code> page again:</p>
<pre><code>HIST_IGNORE_ALL_DUPS
   If a new command line being added to the history list duplicates an
   older one, the older command is removed from the list (even if it
   is not the previous event).

HIST_IGNORE_DUPS (-h)
   Do not enter command lines into the history list if they are
   duplicates of the previous event.

HIST_SAVE_NO_DUPS
   When writing out the history file, older commands that duplicate
   newer ones are omitted.
</code></pre>
<p><code>HIST_IGNORE_DUPS</code> is a subset of <code>HIST_IGNORE_ALL_DUPS</code>, and so the choice is between <code>HIST_SAVE_NO_DUPS</code> and <code>HIST_IGNORE_ALL_DUPS</code>.</p>
<h3 id="hist_save_no_dups"><code>HIST_SAVE_NO_DUPS</code></h3>
<p>Just going by the description and names, <code>HIST_SAVE_NO_DUPS</code> should have worked, but it didn&rsquo;t:</p>
<pre><code>$ setopt HIST_SAVE_NO_DUPS
$ tail -2 ~/.zsh_history
setopt HIST_SAVE_NO_DUPS
tail -2 ~/.zsh_history
$ tail -2 ~/.zsh_history
tail -2 ~/.zsh_history
tail -2 ~/.zsh_history
$ # it's saving duplicates :o
</code></pre>
<p>If I close the above session and view the file in a new session, it removes the duplicate <code>tail -2</code> command:</p>
<pre><code>$ tail -5 ~/.zsh_history
m ~/.zshrc
source ~/.zshrc
setopt HIST_SAVE_NO_DUPS
tail -2 ~/.zsh_history
tail -5 ~/.zsh_history
</code></pre>
<p>I&rsquo;m probably misunderstanding how the option works. I thought &ldquo;writing out the history file&rdquo; meant each time the command got appended, now that I&rsquo;ve set the appending to be that way. But it looks like the removal of duplicates happens only at the end of the session, irrespective of the append behavior.</p>
<p>I looked at the zsh source code for evidence of this, and it turns out this option is only referenced in <a href="https://github.com/zsh-users/zsh/blob/263659acb73d0222e641dfd8d37e48e96582de02/Src/hist.c#L2951">hist.c</a>, in a function called <code>hend</code>, indicating the end of history related operations. This seems like something that would run at the end of a shell session.</p>
<h3 id="hist_ignore_all_dups"><code>HIST_IGNORE_ALL_DUPS</code></h3>
<p>Setting this option seemed to work, sort of.</p>
<pre><code>$ setopt HIST_IGNORE_ALL_DUPS
$ tail -2 ~/.zsh_history
setopt HIST_IGNORE_ALL_DUPS
tail -2 ~/.zsh_history
$ tail -2 ~/.zsh_history
setopt HIST_IGNORE_ALL_DUPS
tail -2 ~/.zsh_history
$ # that worked :D
</code></pre>
<p>While it avoided adding immediate repeated commands (like <code>HIST_IGNORE_DUPS</code>), it removed older instances only once the session was closed (like <code>HIST_SAVE_NO_DUPS</code>).</p>
<p>Then I looked back at the man page, and noticed something I hadn&rsquo;t noticed before: <code>HIST_SAVE_NO_DUPS</code> makes changes to the history &ldquo;file&rdquo;, whereas <code>HIST_IGNORE_ALL_DUPS</code> makes changes to the history &ldquo;list&rdquo;.</p>
<p>How is a history &ldquo;list&rdquo; different from a history &ldquo;file&rdquo;? The history list stores commands for a particular shell session, before they&rsquo;re written to the history file. Keeping in mind the default behavior for saving history, having a temporary list per session makes sense. However, it looks like this list is in use even when the append behavior is changed.</p>
<p>To see how this option affects the list, we can view it using the <code>history</code> command:</p>
<pre><code>$ setopt HIST_IGNORE_ALL_DUPS
$ echo hello
hello
$ echo hello
hello
$ history -2
 1394  setopt HIST_IGNORE_ALL_DUPS
 1395  echo hello
</code></pre>
<p>Commands don&rsquo;t repeat in the history list, and hence aren&rsquo;t repeated in the history file too! To confirm that it works for all older instances of a command, I tried running a command that appears slightly early on in the list:</p>
<pre><code>$ history 0 | grep 'echo test'
 1413  echo test
$ echo test
test
$ history 0 | grep 'echo test'
 1428  echo test
</code></pre>
<p>The line number changed, which means that the older instance was removed from the history list. However, the history file still has the older duplicate:</p>
<pre><code>$ cat ~/.zsh_history | grep '^echo test$'
echo test
echo test
</code></pre>
<p>This is due to the same reason as for why <code>HIST_SAVE_NO_DUPS</code> didn&rsquo;t work - the removal of duplicates from the history file happens only once a shell session ends.</p>
<p>In a nutshell, <code>HIST_IGNORE_ALL_DUPS</code> works like <code>HIST_SAVE_NO_DUPS</code> with the added functionality of removing dupes in the history list. While I expected a shell option to remove older dupes from the file as they were added, this option seems like a resonable alternative.</p>
<h2 id="fin">Fin.</h2>
<p>Phew, that was an unexpectedly long adventure! But my history config has now reduced from 6 lines that I wasn&rsquo;t sure about, to 3 lines that I can confidently reason about!</p>
<pre><code>setopt INC_APPEND_HISTORY
setopt HIST_IGNORE_ALL_DUPS
setopt HIST_IGNORE_SPACE
</code></pre>
]]></description>
    </item>
    
  </channel>
</rss>
