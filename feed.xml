<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pjg1.site</title>
    <link>https://pjg1.site/index.html</link>
    <description>Recent posts on pjg1.site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Nov 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://pjg1.site/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A minimal keyboard key effect with CSS</title>
      <link>https://pjg1.site/kbd-css.html</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/kbd-css.html</guid>
      <description><![CDATA[<p>I use the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/kbd"><code>kbd</code></a> element to specify keypresses in my posts. To differentiate it from the inline <code>code</code> element, I&rsquo;ve styled it as a minimal version of an actual key:</p>
<pre><code>kbd {
  font-family: ui-monospace, monospace;
  font-size: 90%;
  margin: 0 0.07rem;
  padding: 0.07rem 0.35rem;
  border: 0.07rem solid;
  border-bottom: 0.18rem solid;
  border-radius: 0.21rem;
}

kbd:hover {
  border-bottom: 0.07rem solid;
  vertical-align: -0.1rem;
  cursor: text;
}
</code></pre>
<p>The hover effect is inspired from <a href="https://dylanatsmith.com/wrote/styling-the-kbd-element">Styling the kbd element</a> by Dylan Smith. I experimented with the <code>border-bottom</code> and <code>vertical-align</code> values till I found a combination that recreated the hover effect well.</p>
<p>Here are keys from the QWERTY keyboard layout as an example:</p>
<div class="keyboard">
<kbd>Q</kbd><kbd>W</kbd><kbd>E</kbd><kbd>R</kbd><kbd>T</kbd><kbd>Y</kbd><kbd>U</kbd><kbd>I</kbd><kbd>O</kbd><kbd>P</kbd><br>
<kbd>A</kbd><kbd>S</kbd><kbd>D</kbd><kbd>F</kbd><kbd>G</kbd><kbd>H</kbd><kbd>J</kbd><kbd>K</kbd><kbd>L</kbd><br>
<kbd>Z</kbd><kbd>X</kbd><kbd>C</kbd><kbd>V</kbd><kbd>B</kbd><kbd>N</kbd><kbd>M</kbd>
</div>
<style>
div.keyboard {
  text-align: center;
  line-height: 1.7
}
kbd {
  font-family: ui-monospace, monospace;
  font-size: 90%;
  margin: 0 0.07rem;
  padding: 0.07rem 0.35rem;
  border: 0.07rem solid;
  border-bottom: 0.18rem solid;
  border-radius: 0.21rem;
}
kbd:hover {
  border-bottom: 0.07rem solid;
  vertical-align: -0.1rem;
  cursor: text;
}
</style>
]]></description>
    </item>
    
    <item>
      <title>Faded codeblocks using CSS</title>
      <link>https://pjg1.site/fade-block-css.html</link>
      <pubDate>Wed, 26 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/fade-block-css.html</guid>
      <description><![CDATA[<p>One aspect I&rsquo;ve gotten stuck on with the styling of this blog is code blocks. I&rsquo;ve tried adding a border and/or a background color in the past, but I couldn&rsquo;t stick with either of them and constantly kept changing styles.</p>
<p>I wanted a subtler indication to scroll if a block overflows, and a fading gradient towards seemed like a good option.</p>
<p>Here&rsquo;s the CSS I wrote for it:</p>
<pre><code>pre {
  position: relative;
  background: white;
}
pre::after {
  content: &quot;&quot;;
  position: absolute;
  top: 0;
  bottom: 0;
  left: 95%;
  right: 0;
  background-image: linear-gradient(to right, transparent, white);
}
@media (prefers-color-scheme: dark) {
  pre {
    background: black;
  }
  pre::after {
    background-image: linear-gradient(to right, transparent, black);
  }
}
pre code {
  display: block;
  padding: 0.75rem 0;
  overflow: auto;
  padding-inline-end: 1.5rem;
}
</code></pre>
<p>The key behind this effect is the <code>::after</code> pseudo-element, which is a linear-gradient positioned to the right end of the <code>pre</code> block. The <code>left</code> value ensures that the gradient doesn&rsquo;t overlap the block completely, and acts as a subtle gradient, suggesting the user to scroll to see the code.</p>
<p>Usually I&rsquo;d add the scroll to the <code>pre</code> block, however since we want the pseudoelement to stay at a fixed position, the scroll and overflow is applied to the child block - the <code>code</code> element in this case.</p>
<p>The gradient hiding overflow text makes sense, but this also covers the text at the end of the scroll. This is fixed by adding <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/padding-inline-end"><code>padding-inline-end</code></a> to the <code>code</code> element, which adds padding at the end of scroll.</p>
<p>Here&rsquo;s how it looks:</p>
<pre class="block"><code>This is a super long string of text that appears to be faded where the text overflows.</code></pre>
<style>
  pre.block {
    position: relative;
    background: #000;
    color: #fff;
    border: unset;
  }
  pre.block code {
    background: unset;
    border: unset
  }
  pre.block::after {
    content: "";
    position: absolute;
    top: 0;
    bottom: 0;
    left: 95%;
    right: 0;
    background-image: linear-gradient(to right, transparent, #000);
  }
  pre.block code {
    display: block;
    padding: 0.75rem 0;
    overflow: auto;
    padding-inline-end: 1.5rem;
  }
</style>
<p>While I&rsquo;ve demonstrated this for codeblocks, this styling could be extended for any block element - say a paragraph within a div, or a paragraph within a blockquote.</p>
]]></description>
    </item>
    
    <item>
      <title>Baby&#39;s first monitoring system</title>
      <link>https://pjg1.site/first-monitoring-system.html</link>
      <pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/first-monitoring-system.html</guid>
      <description><![CDATA[<p>Till last week, I didn&rsquo;t know what a monitoring system really looked like. A week later, I&rsquo;m in the process of setting up one for the <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a>&rsquo;s shared computing cluster<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  which is community maintained. Here are some notes about the various tools I&rsquo;m using and how they work together (<a href="#tldr">TL;DR</a>).</p>
<p>I&rsquo;m halfway through my second batch at RC, and one of my batch goals was to learn DevOps/SRE skills by contributing to this cluster. Having put it off for the first 5 weeks, I finally reached out to folks in the weekly meeting about the cluster, where I was recommended to look into <a href="https://prometheus.io">Prometheus</a>.</p>
<h2 id="prometheus">Prometheus</h2>
<p>The Prometheus server at its core is a database. More specifically, it is a time-series database, which means it stores key-value pairs with the key being a timestamp, thus showing how a particular value changed over time. This data can be used to create graphs and dashboards or trigger alerts if the values cross a certain threshold (more on both later). It has its own query language called PromQL.</p>
<p>The server can pull and store data from multiple machines, so it runs on only one of the machines in the cluster. However, if this machine goes down for some reason, our monitoring system is down.</p>
<h2 id="node-exporter">Node Exporter</h2>
<p>We have a database, cool, but where does the data come from? There are a variety of tools for this<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, but the one I&rsquo;m using here is Prometheus&rsquo; own tool - <a href="https://github.com/prometheus/node_exporter">Node Exporter</a> - that captures metrics from the system - things like CPU usage, memory usage, filesystem sizes, etc. This runs on each machine in the cluster.</p>
<h2 id="alertmanager">Alertmanager</h2>
<p>The last piece in the puzzle is alerting. The Prometheus server takes in alert rules written in PromQL - things like checking for low disk space, checking if certain services failed to run, high CPU or memory usage, etc.</p>
<p>It checks the metrics against the rules as they arrive. If a rule is met, it sends the alert to a tool called <a href="https://github.com/prometheus/alertmanager">Alertmanager</a> that handles sending of notifications via email or a chat platform. RC uses Zulip for communication, which has an <a href="https://zulip.com/integrations/doc/alertmanager">integration for Alertmanager</a> that I&rsquo;m using in this case.</p>
<h2 id="grafana">Grafana</h2>
<p>Grafana can be integrated with Prometheus to visualize the metrics via graphs and dashboards. I&rsquo;ve mainly been focused on getting alerting to work so far, so I am yet to try making a dashboard.</p>
<h2 id="adding-kubernetes-to-the-mix">Adding Kubernetes to the mix</h2>
<p>The reason I was recommended Prometheus in the first place is because it had been deployed within a Kubernetes cluster by a fellow Recurser.</p>
<p>The above setup would have worked just fine if I ran them as individual services directly on the machine. However, I went ahead with the Kubernetes option for two reasons:</p>
<ol>
<li>I preferred using something that was already deployed over re-inventing the wheel</li>
<li>I&rsquo;d been hearing a lot about Kubernetes, so this would finally be my introduction to the tool</li>
</ol>
<p>One advantage of using Kubernetes is that it makes multiple machines operate as one big unit - you provide it a list of services to deploy, and it&rsquo;ll figure out which machine&rsquo;s resources to utilize and how. Except for Node Exporter which is deployed on all machines, other services like Grafana, Alertmanager and the Prometheus server are deployed automagically.</p>
<p>In my case they&rsquo;re deployed using Kubernetes&rsquo; package manager, Helm. While these eases setup, it also adds some layers of complexity.</p>
<p>Accessing the Prometheus web interface locally now requires more steps than a direct install, as it is isolated from the main system and has its own network and IP address. So multiple port forwards would be required.</p>
<p>Making changes to configuration files is also harder. Kubernetes containers don&rsquo;t have persistent disk space, so it isn&rsquo;t possible to exec into the containers and change files directly<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> like I would with a direct install. So I have to add the configuration to some external file and then pass that file during deployment.</p>
<p>For applications deployed using Helm, I have to modify something called a Helm chart. I don&rsquo;t completely understand what the various files are for, but one of them is <a href="https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml"><code>values.yaml</code></a>, where I would add custom configuration like the alert rules for example. This file is passed to the install command, which then applies the custom config.</p>
<p>This complexity was preventing me from testing stuff quickly, so I decided to break the project into two phases. The first phase was testing Prometheus and alerting using a direct install, which I&rsquo;m almost done with. Once I have a working set of config files, the next phase would be to figure out the Helm install and adding my configuration to the values file.</p>
<h2 id="tldr">TL;DR</h2>
<p>The following diagram is based on my limited understanding of the above concepts.</p>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ node 1        â”‚   â”‚ kubernetes cluster               â”‚
â”‚               â”‚   â”‚                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ node exporter â”‚â—€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”          â”‚ prometheus   â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚          â”‚ server       â”‚ â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚  pull    â”‚              â”‚ â”‚
â”‚ node 2        â”‚   â”‚      â”‚ metrics  â”‚              â”‚ â”‚
â”‚               â”‚   â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚              â”‚ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚      â”‚          â”‚              â”‚ â”‚
â”‚ node exporter â”‚â—€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤          â”‚              â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚          â”‚ alert rules  â”‚ â”‚
â”‚ node 3        â”‚   â”‚      â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚               â”‚   â”‚      â”‚                  â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚      â”‚      rule  â”Œâ”€â”€â”€â”€â”€â”˜        â”‚
â”‚ node exporter â”‚â—€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤      met   â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚            â–¼              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ node 4        â”‚   â”‚      â”‚    â”‚              â”‚  fire alert  â”‚         â”‚
â”‚               â”‚   â”‚      â”‚    â”‚ alertmanager â”‚â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â–¶â”‚  zulip  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚      â”‚    â”‚              â”‚       â”‚      â”‚         â”‚
â”‚ node exporter â”‚â—€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p>Is baby&rsquo;s first monitoring system a bit complex? Yes.</p>
<p>Is baby learning new things and reaching <a href="https://www.recurse.com/self-directives#work-at-the-edge">the edge of their abilities</a> thanks to the complexity? ALSO YES!</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>I knew about the cluster in my first batch, but the thought of contributing to it came to mind only a year later, thanks to some folks from a later batch starting a meeting to discuss stuff relating to the cluster.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>I found that metrics were already being captured on the machines using another tool while working on this. That tool didn&rsquo;t provide any support for alerts though, so I chose to switch to using Prometheus.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Prometheus has a web interface, so I expected that I would be able to change configuration and alert rules from the interface directly. I didn&rsquo;t find a way to do so though, and had to edit the file and restart the service each time.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></description>
    </item>
    
    <item>
      <title>How does a Linux machine connect to the internet, really?</title>
      <link>https://pjg1.site/linux-internet-from-scratch.html</link>
      <pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/linux-internet-from-scratch.html</guid>
      <description><![CDATA[<p>Recently, I was brainstorming networking project ideas, I got curious on what goes behind connecting to the internet, and if I could do it from scratch.</p>
<p>I&rsquo;m delighted to report that the experiment was successful, and I thought of sharing it here! I&rsquo;ve tested this on Ubuntu, but I think it should work on any Linux distribution. If not, <a href="mailto:piya@pjg1.site">let me know</a>.</p>
<ul>
<li><a href="#identify-and-disable-existing-configuration">Identify and disable existing configuration</a></li>
<li><a href="#additional-setup-for-wireless-interfaces">Additional setup for wireless interfaces</a></li>
<li><a href="#setup-network-interface">Setup network interface</a></li>
<li><a href="#set-a-default-gateway">Set a default gateway</a></li>
<li><a href="#setup-dns">Setup DNS</a></li>
<li><a href="#bonus-dynamic-addresses-via-dhcp">Bonus: Dynamic addresses via DHCP</a></li>
</ul>
<h2 id="identify-and-disable-existing-configuration">Identify and disable existing configuration</h2>
<p>Before I could set stuff up manually, I had to figure out my machine&rsquo;s existing configuration and disable it, so it wouldn&rsquo;t interfere with my handcrafted setup.</p>
<p>The <a href="https://documentation.ubuntu.com/server/explanation/networking/configuring-networks/">Ubuntu documentation</a> was a useful resource to find out the services in use. The network on my machine is configured using NetworkManager and DNS is managed using the systemd-resolved service.</p>
<p>I figured out what the above tools had setup using by trying out some of the code snippets in the docs, so I had a plan and a final result in mind.</p>
<p>Based on this, I made a note of the following from the existing configuration, which can be found by running <code>ip addr show</code>:</p>
<ul>
<li>Interface name - typically starts with one of <code>eth</code>, <code>en</code>, <code>wlan</code> or <code>wl</code>.</li>
<li>IP address associated with the interface</li>
<li>Subnet mask - the slash next to the IP address</li>
</ul>
<p>Once I had the information noted down, I disabled<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> NetworkManager and systemd-resolved (both running as <code>systemd</code> services) and set the network interface to down:</p>
<pre><code># systemctl stop NetworkManager
# systemctl disable NetworkManager
Removed &quot;/etc/systemd/system/network-online.target.wants/NetworkManager-wait-online.service&quot;.
Removed &quot;/etc/systemd/system/multi-user.target.wants/NetworkManager.service&quot;.
Removed &quot;/etc/systemd/system/dbus-org.freedesktop.nm-dispatcher.service&quot;.
# systemctl stop systemd-resolved
# systemctl disable systemd-resolved
# ip link set dev wlp3s0 down
</code></pre>
<p>With this, the machine is no longer connected to the internet.</p>
<h2 id="additional-setup-for-wireless-interfaces">Additional setup for wireless interfaces</h2>
<p>There are two types of interfaces you could be setting up.</p>
<p>One is for a connection made by connecting an Ethernet cable to your machine. If you were to try out this post on a Linux VM, you would be setting up an Ethernet connection and can skip this section. The other is a wireless interface, which can connect to WiFi networks.</p>
<p>An Ethernet interface appears up/enabled at all times - even before it has actual internet access - as its connected via cable. Wireless interfaces on the other hand remain down/disabled until you connect to a WiFi network.</p>
<p>This led to differences during setup, which required me to add separate instructions for both, making the post long and confusing.</p>
<p>It is possible to connect to a WiFi network before having internet access - this would be similar to situations when your phone or laptop displays a &ldquo;No Internet Connection&rdquo; message while being connected to a network.</p>
<p>The tool that helps connect to WiFi is <a href="https://www.linuxfromscratch.org/blfs/view/stable-systemd/basicnet/wpa_supplicant.html"><code>wpa_supplicant</code></a>. This is what the previous setup used, so I went with it. There may be a process for it running in the background from the previous setup which is no longer required, so you can terminate it if it exists:</p>
<pre><code># ps -ef | grep -i [w]pa
root         883       1  0 01:59 ?        00:00:00 /usr/sbin/wpa_supplicant -u -s -O DIR=/run/wpa_supplicant GROUP=netdev
# systemctl stop wpa_supplicant
# systemctl disable wpa_supplicant
</code></pre>
<p>The tool takes a configuration file, <code>wpa_supplicant.conf</code>, which contains information about the WiFi network you wish to connect to.</p>
<pre><code># cat &lt;&lt;EOF &gt; /etc/wpa_supplicant/wpa_supplicant.conf
ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
update_config=1

network={
    ssid=&quot;&lt;name&gt;&quot;
    psk=&quot;&lt;password&gt;&quot;
}
EOF
</code></pre>
<p>Replace <code>&lt;name&gt;</code> and <code>&lt;password&gt;</code> with your WiFi&rsquo;s name and password in plaintext. Yes, you read that right - a PASSWORD stored in PLAINTEXT<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. I&rsquo;m pretty shocked by this, but it seems to be a norm for WiFi tools, not sure why<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>I was following <a href="https://ubuntuforums.org/showthread.php?t=571188">this tutorial</a> which added further details to the network block like the protocol type and the encryption used. However, adding just the username and password seemed to work in my case.</p>
<p>Then, I ran <code>wpa_supplicant</code> with the config file:</p>
<pre><code># wpa_supplicant -D nl80211 -i wlp3s0 -c /etc/wpa_supplicant/wpa_supplicant.conf -B
Successfully initialized wpa_supplicant
</code></pre>
<p>This is run as a background process (<code>-B</code>) so I can continue using the terminal to type other commands. I can confirm if the connection took place successfully via <code>iw</code>:</p>
<pre><code># iw dev wlp3s0 info
Interface wlp3s0
    ifindex 2
    wdev 0x1
    addr &lt;MAC&gt;
    ssid &lt;name&gt;
    type managed
    wiphy 0
...
</code></pre>
<p>If the name next to the <code>ssid</code> field matches with name set in the configuration, that means the connection was successful.</p>
<h2 id="setup-network-interface">Setup network interface</h2>
<p>Without internet access, my network interface looked like this:</p>
<pre><code># ip addr show dev wlp3s0
2: wlp3s0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000
    link/ether &lt;MAC&gt; brd ff:ff:ff:ff:ff:ff
</code></pre>
<p>It needed an IP address to be able to talk to other machines that was missing. I assigned it one based on the information I noted down from the previous setup:</p>
<pre><code># ip addr add 192.168.100.128/24 dev wlp3s0
</code></pre>
<p><code>192.168.100.128</code> is the IP address and <code>/24</code> is the subnet. The subnet, - a shorthand for <code>255.255.255.0</code> - means that this network assigns addresses in the range of <code>192.168.100.X</code>, where <code>X</code> can be anywhere between 1 and 254 (0 and 255 are reserved).</p>
<p>I checked the interface after setting it to up, after which I can see the address!</p>
<pre><code># ip link set dev wlp3s0 up
# ip addr show dev wlp3s0
2: wlp3s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether &lt;MAC&gt; brd ff:ff:ff:ff:ff:ff
    inet 192.168.100.128/24 scope global wlp3s0
       valid_lft forever preferred_lft forever
</code></pre>
<p>With this, I was online!</p>
<p>Well, sort of. If I tried to ping another machine in the same network, it worked!</p>
<pre><code># ping -c3 192.168.100.141
PING 192.168.100.141 (192.168.100.141) 56(84) bytes of data.
64 bytes from 192.168.100.141: icmp_seq=1 ttl=64 time=4.09 ms
64 bytes from 192.168.100.141: icmp_seq=2 ttl=64 time=92.1 ms
64 bytes from 192.168.100.141: icmp_seq=3 ttl=64 time=113 ms

--- 192.168.100.141 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 4.088/69.678/112.835/47.144 ms
</code></pre>
<p>However, pinging a machine outside the network didn&rsquo;t.</p>
<pre><code># ping 1.1.1.1
ping: connect: Network is unreachable
</code></pre>
<p>There had to be a way to route packets outside of the network.</p>
<h2 id="set-a-default-gateway">Set a default gateway</h2>
<p>Accessing machines outside of the network requires a default gateway - an address that forwards packets to other networks when the destination address isn&rsquo;t part of the network&rsquo;s address range. In a home network, this address would likely be assigned to your router.</p>
<p>This information is added to the routing table, and is typically the first assignable address in the address range, <code>192.168.100.1</code> in this case. The default gateway was set using <code>ip route</code>:</p>
<pre><code># ip route add default via 192.168.100.1 dev wlp3s0
# ip route show
default via 192.168.100.1 dev wlp3s0
192.168.100.0/24 wlp3s0 proto kernel scope link src 192.168.100.128
</code></pre>
<p><code>ip route show</code> displays the routing table. The first rule is the one I just set, and the second one specifies routing for the entire address range, which was set after I assigned the address in the previous step.</p>
<p>Pinging to addresses outside of the network now worked!</p>
<pre><code># ping -c3 1.1.1.1
PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data.
64 bytes from 1.1.1.1: icmp_seq=1 ttl=59 time=23.4 ms
64 bytes from 1.1.1.1: icmp_seq=2 ttl=59 time=8.74 ms
64 bytes from 1.1.1.1: icmp_seq=3 ttl=59 time=7.11 ms

--- 1.1.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 7.113/13.093/23.426/7.336 ms
</code></pre>
<p>But if I were to try pinging a domain name, that wouldn&rsquo;t work.</p>
<pre><code># ping example.com
ping: example.com: Temporary failure in name resolution
</code></pre>
<p>So close, yet so far. The error message meant that it is unable to translate example.com to an IP address, which points towards a DNS issue.</p>
<h2 id="setup-dns">Setup DNS</h2>
<p>The process of translating domain names to IP addresses is done by a nameserver. These name servers are defined in <code>/etc/resolv.conf</code>, which on my machine was a symbolic link:</p>
<pre><code># ls -l /etc/resolv.conf
lrwxrwxrwx 1 root root 39 Feb  7 04:49 /etc/resolv.conf -&gt; ../run/systemd/resolve/stub-resolv.conf
</code></pre>
<p>This was part of the previous configuration, as DNS was setup using systemd-resolved on this machine. Since I&rsquo;ve disabled that, I removed the symlink and added my nameservers of choice. I used <a href="https://www.cloudflare.com/learning/dns/what-is-1.1.1.1/">Cloudflare&rsquo;s public DNS server</a> in this case:</p>
<pre><code># rm /etc/resolv.conf
# cat &lt;&lt;EOF &gt; /etc/resolv.conf
nameserver 1.1.1.1
nameserver 1.0.0.1
EOF
</code></pre>
<p>Pinging domain names finally worked!</p>
<pre><code># ping -c 1 example.com
PING example.com (96.7.128.198) 56(84) bytes of data.
64 bytes from a96-7-128-198.deploy.static.akamaitechnologies.com (96.7.128.198): icmp_seq=1 ttl=51 time=269 ms

--- example.com ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 269.178/269.178/269.178/0.000 ms
</code></pre>
<p>This brings mission &ldquo;connect to the Internet from scratch&rdquo; to an end! I had a lot of fun working on this and learnt a lot, I hope you enjoyed reading this too! Before I end the post though, there&rsquo;s one little side quest I wanted to cover.</p>
<h2 id="bonus-dynamic-addresses-via-dhcp">Bonus: Dynamic addresses via DHCP</h2>
<p>The IP address I set above is a static IP, which doesn&rsquo;t change. Each time I connect to the network, I can assign it the same address.</p>
<p>There are two problems with this:</p>
<ul>
<li>I need to know the address range for each network before I connect to it, which is time-consuming.</li>
<li>Setting an IP this way might cause confusion if another machine has been assigned the same address.</li>
</ul>
<p>The solution for this is to let the network assign an address when you connect to it, which is how your default setup most likely works. This is done using DHCP or the Dynamic Host Configuration Protocol.</p>
<p>I got it working using a tool called <code>dhclient</code>. It doesn&rsquo;t work if an IP address is already assigned to the interface, so I removed the static IP and default gateway I had set first:</p>
<pre><code># ip addr flush dev wlp3s0
# ip route flush dev wlp3s0
# dhclient -v wlp3s0
Internet Systems Consortium DHCP Client 4.4.3-P1
Copyright 2004-2022 Internet Systems Consortium.
All rights reserved.
For info, please visit https://www.isc.org/software/dhcp/

Listening on LPF/wlp3s0/&lt;MAC&gt;
Sending on   LPF/wlp3s0/&lt;MAC&gt;
Sending on   Socket/fallback
xid: warning: no netdev with useable HWADDR found for seed's uniqueness enforcement
xid: rand init seed (0x67d6369b) built using gethostid
DHCPDISCOVER on wlp3s0 to 255.255.255.255 port 67 interval 3 (xid=0xf29dbc1c)
DHCPOFFER of 192.168.100.128 from 192.168.100.1
DHCPREQUEST for 192.168.100.128 on wlp3s0 to 255.255.255.255 port 67 (xid=0x1cbc9df2)
DHCPACK of 192.168.100.128 from 192.168.100.1 (xid=0xf29dbc1c)
bound to 192.168.100.128 -- renewal in 271244 seconds.
</code></pre>
<p>From the output, it looks like the network&rsquo;s router (<code>192.168.100.1</code>) assigned this machine with the address <code>192.168.100.128</code>, which is what I was setting statically too.</p>
<p>What I also noticed was that running this also setup the default gateway and DNS automagically - and that too to the same address?!?!?</p>
<pre><code># ip route show | awk '/default via/{print $3}'
192.168.100.1
# cat /etc/resolv.conf
192.168.100.1
</code></pre>
<p>After some searching, I <a href="https://lobste.rs/s/563zjp/how_does_linux_machine_connect_internet">found</a> that my router (aka the default gateway) is also capable of handling DNS requests. More specifically, it can forward DNS requests to servers it has configured that are likely specified by my ISP, and then send it back to my machine. Pretty cool!</p>
<p>What&rsquo;s not so cool though, is that this one command basically automated almost everything I set up lovingly by hand :/ The experiment was still worth it though, as I now know exactly what steps the tool is automating.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>In my first attempt, I removed NetworkManager from the system all together, but reached a dead end and had to reinstall it. That&rsquo;s why I recommend disabling instead, as its easy to start over by enabling the service.Â &#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>It is possible to generate a password hash with a tool called <code>wpa_passphrase</code>, but turns out that you can use the hash as is to connect to a network without knowing the actual password. This kind of makes hashing pointless.Â &#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Even NetworkManager had my WiFi password stored in plaintext in a config file, which was a shocker. The rationale provided is that the file permissions are set such that only root can access it, making it safe. I&rsquo;m not so sure about that.Â &#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></description>
    </item>
    
    <item>
      <title>Things I learnt while working on ZulipFS</title>
      <link>https://pjg1.site/zulipfs.html</link>
      <pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/zulipfs.html</guid>
      <description><![CDATA[<p>I came across <a href="https://en.wikipedia.org/wiki/Filesystem_in_Userspace">FUSE</a> recently, which brought up an idea: <em>what if I could access a <a href="https://zulip.com">Zulip</a> instance as a filesystem?</em></p>
<p>This led to the creation of ZulipFS, where channels are represented as directories, and topics as files within those directories. The usage and code for the project is available <a href="https://github.com/pjg11/zulipfs">here</a>. This post talks about things I learnt about FUSE, filesystems in general and design choices I made in the process.</p>
<h2 id="how-fuse-works-in-a-nutshell">How FUSE works in a nutshell</h2>
<p>FUSE lets you mount a folder containing files and folders. These could be actual files and folders (eg: <a href="https://github.com/libfuse/sshfs">SSHFS</a> where files from a remote network are mounted) or virtual files (eg: <a href="https://omar.website/tabfs/">TabFS</a>, where information from browser tabs are presented as files and folders). You write your own implementation for relevant <a href="https://libfuse.github.io/doxygen/structfuse__operations.html">system calls</a> relating to file operations, and FUSE will run your implementation instead of the standard implementation.</p>
<p>The system calls I implemented in ZulipFS are:</p>
<ul>
<li><code>read</code> + <code>write</code> for reading and writing files</li>
<li><code>readdir</code> to list directory information</li>
<li><code>getattr</code> that returns the metadata for each file and directory</li>
</ul>
<h2 id="file-metadata-is-more-important-than-i-thought">File metadata is more important than I thought</h2>
<p>When I implemented and tested reading a message from a topic, only part of a message got printed. I checked if the API call was returning a partial message, but that seemed fine.</p>
<p>Turns out, I had the file size set to 512 bytes as I was working off of <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/useful/dnsfs.py">this example code</a>. So <code>read</code> checks for the size of a file and prints only that many bytes, which makes sense!</p>
<p>I now had to figure out a way to get the file size before the message has been read.</p>
<h2 id="reading-and-knowing-a-file-are-two-different-things">Reading and knowing a file are two different things</h2>
<p>The file size is set in the <code>getattr</code> function, which is called each time a file is read or listed. So the Zulip API would be called twice for a file read - once in <code>getattr</code> to get the length of the message, and then in <code>read</code> to display the message itself.</p>
<p>For this, I created a function called <code>get_topic</code> which can be called by <code>getattr</code> and <code>read</code>:</p>
<pre><code>def get_topic(self, channel, topic):
  channel_id = self.channels[channel]['stream_id']

  # returns the ID of the last message along with the topic name
  topicslist = self.client.get_stream_topics(channel_id)['topics']
  self.topics[channel] = { self.normalize(t['name']): t for t in topicslist }

  # get the message contents using the message ID
  message = self.client.get_raw_message(self.topics[channel][topic]['max_id'])
  message_fmt = f&quot;&quot;&quot;[{datetime.fromtimestamp(message['message']['timestamp'])}] {message['message']['sender_full_name']}
{message['raw_content']}
&quot;&quot;&quot;.encode()

  self.topics[channel][topic] = {
    'last_message': message_fmt,
    'last_timestamp': float(message['message']['timestamp']),
  }
  return self.topics[channel][topic]
</code></pre>
<p>Making the same API calls twice felt a bit excessive for reading, but it was okay as long as it wasn&rsquo;t slowing things down. Then I tried listing all the topics in a channel via <code>ls</code>, and things slowed downâ€¦A LOT.</p>
<p>Why, you ask? The function handling directory listing, <code>readdir</code>, calls <code>getattr</code> for EACH FILE in the directory. If a channel has 300 topics, that&rsquo;s 300 API calls before <code>ls</code> completes execution. To add to the chaos, <code>get_topic</code> above uses API calls instead of one, which means 600 API calls before <code>ls</code> completes execution. I had to find ways to optimize this.</p>
<h2 id="lazy-loading-files">Lazy loading files?!?!?</h2>
<p>The first optimization attempt was to remove the <code>get_topic</code> call from <code>getattr</code>, and call it only in <code>read</code>. I placed an exception block in <code>getattr</code>, which would assign a file size of 65535 bytes on mount, and a subsequent <code>read</code> would fill the hash map with the correct values, which <code>getattr</code> would take the next time its called.</p>
<pre><code>def getattr(self, path):
# ...snip...
# topic/file
try:
  channel, topic = path[1:].split('/')
  try:
    timestamp = self.topics[channel][topic]['last_timestamp']
  except KeyError:
    timestamp = now

  try:
    st.st_size = len(self.topics[channel][topic]['last_message'])
  except KeyError:
    st.st_size = 65535
# ...snip...
</code></pre>
<p>This worked initially, but caused problems when I wanted to append new messages instead of just displaying the last one. After lots of trial and error, a question popped up in my head: <em>What if I don&rsquo;t create all topic files right away, and add them only after someone tries to read or list it?</em></p>
<p>This seemed like a great idea as it would significantly reduce the number of API calls made at once. Things might slow down eventually as you read more and more topics, but it would still be faster than trying to list all topics at once.</p>
<p>Another optimization I was able to make was combining the two API calls into one using <a href="https://zulip.com/api/get-messages"><code>get_messages</code></a>, which powers Zulip&rsquo;s search functionality. I can pass it the name of a channel and topic, and ask it to return the last message of that topic. If either the channel or topic doesn&rsquo;t exist, it&rsquo;ll return an empty result.</p>
<pre><code>def get_topic(self, channel, topic):
  request = {
    &quot;anchor&quot;: &quot;newest&quot;,
    &quot;num_before&quot;: 1,
    &quot;num_after&quot;: 0,
    &quot;narrow&quot;: [
      {&quot;operator&quot;: &quot;channel&quot;, &quot;operand&quot;: self.channels[channel]['name']},
      {&quot;operator&quot;: &quot;topic&quot;, &quot;operand&quot;: self.zulip_name(topic)},
    ],
    &quot;apply_markdown&quot;: False,
  }
  try:
    message = self.client.get_messages(request)['messages'][0]
    message_fmt = f&quot;&quot;&quot;[{datetime.fromtimestamp(message['timestamp'])}] {message['sender_full_name']}
{message['content']}
&quot;&quot;&quot;.encode()

    self.topics[channel][topic] = {
      'last_message': message_fmt,
      'last_timestamp': float(message['timestamp']),
    }
  except IndexError:
    # channel or topic doesn't exist
    pass

  # if a channel or topic doesn't exist, this statement will cause an
  # exception in the function where this is called.
  return self.topics[channel][topic]
</code></pre>
<p>These optimizations made things fast enough that I could call <code>get_topic</code> from <code>getattr</code> again, so I could get rid of the extra try/except blocks:</p>
<pre><code># topic/file
try:
  channel, topic = path[1:].split('/')
  t = self.get_topic(channel, topic)
  st.st_mode = stat.S_IFREG | 0o644
  st.st_nlink = 1
  st.st_size = len(t['last_message'])
  st.st_mtime = t['last_timestamp']
except (KeyError, ValueError):
  return -errno.ENOENT
</code></pre>
<h2 id="appending-new-messages">Appending new messages</h2>
<p>I presented the pre-optimization version at the weekly <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a> presentations, and fellow batchmates <a href="https://eieio.games">Nolen</a> and <a href="https://bsky.app/profile/ohsh.it">Kevin O</a> suggested to add the ability to read new messages from a topic as they arrive by running <code>tail -f</code> on the file. This seemed like a good idea, and more useful than displaying just the last message.</p>
<p>I initially thought appending would require implementing a system call, but it was easier than I thought - if the timestamp of the current message is newer than the previous one, I append the new message to the end of the previous one in <code>get_topic</code>. I also needed an additional check for whether the topic had been read before or not, to initialize the file for the first time.</p>
<p>The <code>try</code> block in <code>get_topic</code> now looks like this:</p>
<pre><code>try:
  message = self.client.get_messages(request)['messages'][0]
  timestamp = float(message['timestamp'])
  message_fmt = f&quot;&quot;&quot;[{datetime.fromtimestamp(message['timestamp'])}] {message['sender_full_name']}
{message['content']}
&quot;&quot;&quot;.encode()

  if topic not in self.topics[channel]:
    # First message in file
    self.topics[channel][topic] = {
      'last_message': message_fmt,
      'last_timestamp': timestamp,
    }
    else:
      # Subsequent messages appended to file
      if timestamp &gt; self.topics[channel][topic]['last_timestamp']:
        self.topics[channel][topic] = {
          'last_message': self.topics[channel][topic]['last_message'] + b&quot;\n&quot; + message_fmt,
          'last_timestamp': timestamp,
        }
except IndexError:
  # channel or topic doesn't exist
  pass
</code></pre>
<h2 id="filename-gotchas">Filename gotchas</h2>
<p>One of the earliest errors I encountered was displaying names that had slashes in them. In Linux and other Unix-based OS&rsquo;s, a slash is considered as a delimiter for a directory rather than part of a filename. One thing I&rsquo;d seen certain apps do is change special characters to their URL-encoded versions, so I replaced all instances of <code>/</code> with <code>%2F</code>.</p>
<p>Another set of characters that are inconvenient to type in the terminal are emojis. I was initially thinking of getting rid of them, but then I realized that looking up the channel name would become tricky.</p>
<p>I remembered seeing textual representations of emojis, which turns out are called <a href="https://emojipedia.org/shortcodes">shortcodes</a>, and they&rsquo;re written as text in-between colons <code>:</code>. For example, the shortcode for ğŸ“ is <code>:memo:</code>â€‹, and these are understood by Zulip. Python has an <code>emoji</code> package that converts emojis to shortcodes and vice versa.</p>
<p>With that, I had two functions to convert Zulip names to a valid filename and vice versa.</p>
<pre><code>def file_name(self, name):
  return emoji.demojize(name.replace('/', '%2F'))

def zulip_name(self, name):
  return emoji.emojize(name.replace('%2F', '/'))
</code></pre>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to <a href="https://fractalkitty.com">Sophia</a> for reviewing a draft of this post.</p>
]]></description>
    </item>
    
    <item>
      <title>Making sense of zsh history shell options</title>
      <link>https://pjg1.site/zsh-history-opts.html</link>
      <pubDate>Thu, 09 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/zsh-history-opts.html</guid>
      <description><![CDATA[<p>I was refactoring my <code>.zshrc</code> recently, and found these existing options for managing the command history:</p>
<pre><code>setopt INC_APPEND_HISTORY
setopt SHARE_HISTORY
setopt HIST_IGNORE_DUPS
setopt HIST_IGNORE_ALL_DUPS
setopt HIST_SAVE_NO_DUPS
setopt HIST_IGNORE_SPACE
</code></pre>
<p>I think I copied these from somewhere whenever I last modified the file. But this time, I wanted to apply a more systematic approach to setting options, where I was sure I knew exactly what each option in the file was doing.</p>
<p>I stared at these options for a bit. Some of these look similar to each other, and I don&rsquo;t know how each one is different despite comments. Do I even need all of these?</p>
<p>For the purposes of this article, I&rsquo;ll be dividing the options into categories:</p>
<ul>
<li>Appending history - <code>SHARE_HISTORY</code>, <code>INC_APPEND_HISTORY</code></li>
<li>Managing duplicates - <code>HIST_IGNORE_DUPS</code>, <code>HIST_IGNORE_ALL_DUPS</code>, <code>HIST_SAVE_NO_DUPS</code></li>
<li><code>HIST_IGNORE_SPACE</code> makes sense right away, so I&rsquo;ll add it to my config directly.</li>
</ul>
<h2 id="appending-history">Appending history</h2>
<p>The default behavior for writing to the history file is to write all commands from a session in bulk at the end of the session. I&rsquo;m looking for a way to append commands from different sessions as they&rsquo;re entered to the history file, aka sharing the same file across sessions.</p>
<p>The solution for this was simple, <code>man zshoptions</code>:</p>
<pre><code>APPEND_HISTORY &lt;D&gt;
   If this is set, zsh sessions will append their history list to the
   history file, rather than replace it. Thus, multiple parallel zsh
   sessions will all have the new entries from their history lists
   added to the history file, in the order that they exit. [...]

INC_APPEND_HISTORY
   This option works like APPEND_HISTORY except that new history lines
   are added to the $HISTFILE incrementally (as soon as they are
   entered), rather than waiting until the shell exits.

SHARE_HISTORY &lt;K&gt;
   This option both imports new commands from the history file, and
   also causes your typed commands to be appended to the history file
   (the latter is like specifying INC_APPEND_HISTORY, which should be
   turned off if this option is in effect).
</code></pre>
<p>One of the first things that stood out was the fact that only one of these options needs to be set. <code>INC_APPEND_HISTORY</code> has the functionality of <code>APPEND_HISTORY</code> and part of <code>SHARE_HISTORY</code> works like <code>INC_APPEND_HISTORY</code>.</p>
<p><code>APPEND_HISTORY</code> didn&rsquo;t do what I wanted, so it was up to me to decide of making a choice between the latter two.</p>
<p>I&rsquo;m primarily concered around writing commands to the file, so that they&rsquo;re available in any sessions I start after it, not so much existing shell sessions (which <code>SHARE_HISTORY</code> does), so I chose <code>INC_APPEND_HISTORY</code>.</p>
<p>Another option I found in the <code>man</code> page was <code>INC_APPEND_HISTORY_TIME</code>, which works like <code>INC_APPEND_HISTORY</code> but appends the commands to the file once they&rsquo;ve completed, which I thought was cool.</p>
<h2 id="managing-duplicates">Managing duplicates</h2>
<p>The default behavior is to keep duplicates. I&rsquo;m looking for a way to store only the most recent version of a command and delete all instances of it from the file, as the command is entered.</p>
<p>Starting with the <code>man</code> page again:</p>
<pre><code>HIST_IGNORE_ALL_DUPS
   If a new command line being added to the history list duplicates an
   older one, the older command is removed from the list (even if it
   is not the previous event).

HIST_IGNORE_DUPS (-h)
   Do not enter command lines into the history list if they are
   duplicates of the previous event.

HIST_SAVE_NO_DUPS
   When writing out the history file, older commands that duplicate
   newer ones are omitted.
</code></pre>
<p><code>HIST_IGNORE_DUPS</code> is a subset of <code>HIST_IGNORE_ALL_DUPS</code>, and so the choice is between <code>HIST_SAVE_NO_DUPS</code> and <code>HIST_IGNORE_ALL_DUPS</code>.</p>
<h3 id="hist_save_no_dups"><code>HIST_SAVE_NO_DUPS</code></h3>
<p>Just going by the description and names, <code>HIST_SAVE_NO_DUPS</code> should have worked, but it didn&rsquo;t:</p>
<pre><code>$ setopt HIST_SAVE_NO_DUPS
$ tail -2 ~/.zsh_history
setopt HIST_SAVE_NO_DUPS
tail -2 ~/.zsh_history
$ tail -2 ~/.zsh_history
tail -2 ~/.zsh_history
tail -2 ~/.zsh_history
$ # it's saving duplicates :o
</code></pre>
<p>If I close the above session and view the file in a new session, it removes the duplicate <code>tail -2</code> command:</p>
<pre><code>$ tail -5 ~/.zsh_history
m ~/.zshrc
source ~/.zshrc
setopt HIST_SAVE_NO_DUPS
tail -2 ~/.zsh_history
tail -5 ~/.zsh_history
</code></pre>
<p>I&rsquo;m probably misunderstanding how the option works. I thought &ldquo;writing out the history file&rdquo; meant each time the command got appended, now that I&rsquo;ve set the appending to be that way. But it looks like the removal of duplicates happens only at the end of the session, irrespective of the append behavior.</p>
<p>I looked at the zsh source code for evidence of this, and it turns out this option is only referenced in <a href="https://github.com/zsh-users/zsh/blob/263659acb73d0222e641dfd8d37e48e96582de02/Src/hist.c#L2951">hist.c</a>, in a function called <code>hend</code>, indicating the end of history related operations. This seems like something that would run at the end of a shell session.</p>
<h3 id="hist_ignore_all_dups"><code>HIST_IGNORE_ALL_DUPS</code></h3>
<p>Setting this option seemed to work, sort of.</p>
<pre><code>$ setopt HIST_IGNORE_ALL_DUPS
$ tail -2 ~/.zsh_history
setopt HIST_IGNORE_ALL_DUPS
tail -2 ~/.zsh_history
$ tail -2 ~/.zsh_history
setopt HIST_IGNORE_ALL_DUPS
tail -2 ~/.zsh_history
$ # that worked :D
</code></pre>
<p>While it avoided adding immediate repeated commands (like <code>HIST_IGNORE_DUPS</code>), it removed older instances only once the session was closed (like <code>HIST_SAVE_NO_DUPS</code>).</p>
<p>Then I looked back at the man page, and noticed something I hadn&rsquo;t noticed before: <code>HIST_SAVE_NO_DUPS</code> makes changes to the history &ldquo;file&rdquo;, whereas <code>HIST_IGNORE_ALL_DUPS</code> makes changes to the history &ldquo;list&rdquo;.</p>
<p>How is a history &ldquo;list&rdquo; different from a history &ldquo;file&rdquo;? The history list stores commands for a particular shell session, before they&rsquo;re written to the history file. Keeping in mind the default behavior for saving history, having a temporary list per session makes sense. However, it looks like this list is in use even when the append behavior is changed.</p>
<p>To see how this option affects the list, we can view it using the <code>history</code> command:</p>
<pre><code>$ setopt HIST_IGNORE_ALL_DUPS
$ echo hello
hello
$ echo hello
hello
$ history -2
 1394  setopt HIST_IGNORE_ALL_DUPS
 1395  echo hello
</code></pre>
<p>Commands don&rsquo;t repeat in the history list, and hence aren&rsquo;t repeated in the history file too! To confirm that it works for all older instances of a command, I tried running a command that appears slightly early on in the list:</p>
<pre><code>$ history 0 | grep 'echo test'
 1413  echo test
$ echo test
test
$ history 0 | grep 'echo test'
 1428  echo test
</code></pre>
<p>The line number changed, which means that the older instance was removed from the history list. However, the history file still has the older duplicate:</p>
<pre><code>$ cat ~/.zsh_history | grep '^echo test$'
echo test
echo test
</code></pre>
<p>This is due to the same reason as for why <code>HIST_SAVE_NO_DUPS</code> didn&rsquo;t work - the removal of duplicates from the history file happens only once a shell session ends.</p>
<p>In a nutshell, <code>HIST_IGNORE_ALL_DUPS</code> works like <code>HIST_SAVE_NO_DUPS</code> with the added functionality of removing dupes in the history list. While I expected a shell option to remove older dupes from the file as they were added, this option seems like a resonable alternative.</p>
<h2 id="fin">Fin.</h2>
<p>Phew, that was an unexpectedly long adventure! But my history config has now reduced from 6 lines that I wasn&rsquo;t sure about, to 3 lines that I can confidently reason about!</p>
<pre><code>setopt INC_APPEND_HISTORY
setopt HIST_IGNORE_ALL_DUPS
setopt HIST_IGNORE_SPACE
</code></pre>
]]></description>
    </item>
    
    <item>
      <title>Re-thinking the way I manage personal projects</title>
      <link>https://pjg1.site/personal-proj-mgmt.html</link>
      <pubDate>Fri, 27 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/personal-proj-mgmt.html</guid>
      <description><![CDATA[<p>I have been trying a new way to keeping track of projects, and I went down a fun rabbit hole before settling on an approach, that I thought would be fun to share.</p>
<h2 id="but-first-some-context">But first, some context</h2>
<p>I&rsquo;ve been maintaining a <a href="https://stackoverflow.blog/2024/05/22/you-should-keep-a-developer-s-journal/">developer journal</a> for the past few months. I create a note each day and write down things I learnt as I&rsquo;m working on them, or use it to break down and keep track of tasks when I&rsquo;m stuck. If I&rsquo;m working on multiple projects in the same day, they all go in the same daily note.</p>
<p>Recently, I came across the concept of <a href="https://nesslabs.com/interstitial-journaling">interstitial journaling</a>, where you write stuff throughout the day, but with a timestamp before each message. It seemed like a good idea as it gives me a better sense of how my day went, and was easy to incorporate with my then writing app of choice, <a href="http://logseq.com/">Logseq</a>.</p>
<p>While looking for Logseq plugins, I came across the following comment on a <a href="https://www.reddit.com/r/logseq/comments/tdysnj/comment/k0skrb8/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button">Reddit thread</a>:</p>
<blockquote>
<p>This is why I used personal IRC and Discord servers/channels for logging/journaling. Just start typing; all entries are automatically time-stamped, sorted, and cloud-backed.</p></blockquote>
<p>My mind was BLOWN. The idea of a chat platform as a means of logging personal projects made so much sense! I had basically been trying to recreate the feeling of chatting with myself, and I didn&rsquo;t realize it until I read this comment.</p>
<p>I had been coming across the idea of <a href="https://github.com/awesome-selfhosted/awesome-selfhosted">self-hosting</a> while looking for project ideas and saw IRC mentioned there, so I looked into it. <strong>That&rsquo;s where the idea clicked</strong>.</p>
<h2 id="channels-as-project-logs">Channels as project logs</h2>
<p>Trying out IRC felt like using something like Slack or Discord, but much more minimal. It is a text-based chat protocol based on the client-server model. There are servers that consist of one or more channels, and clients with which you can connect to and interact on channels by joining them.</p>
<p>The <a href="https://weechat.org/about/screenshots/weechat/weechat_2010-02-22_hullap.png/">interface</a> is also a minimal version of Slack or Discord, with a list of channels you&rsquo;ve joined on the left and time-stamped messages from a channel in the center.</p>
<p>Now how does this relate to project management, you ask?</p>
<ul>
<li>Each channel represents one project I&rsquo;m working on</li>
<li>When I want to log something specific to a project, I post a message in that channel</li>
</ul>
<p>This simple concept solves issues I&rsquo;ve faced with the existing dev journal setup.</p>
<p>When I revisit a project after a break (which I often do), that would require searching through older notes to figure out where I last left off. While the daily notes made things easier on a day-to-day basis, I often felt the lack of a overall view of a project.</p>
<p>A channel helps with both time-tracking and project management:</p>
<ul>
<li>The messages are time-stamped, so I know how my day went</li>
<li>The channel serves as the overall picture of my project - what I&rsquo;ve done, what I need to do and where I last left off. Everything relating to a project is in one place.</li>
<li>The channels list in the side tells me at a glance which projects I&rsquo;m working on, and lets me easily switch between them.</li>
</ul>
<p>This also helps in cases when I want to write a post about something I did over a long period of time - say weeks - and I forget details of what I did earlier.</p>
<p>A chat interface encourages me to write smaller updates, so I end up with a reliable log of events, as opposed to me having logged only certain aspects and trying to remember other details while writing the post (extremely annoying, trust me). And <a href="https://pjg1.site/writing-is-thinking">more writing leads to more thinking</a>, so I consider this approach a win!</p>
<p>So I have the idea set, now comes the rabbit hole I went into while trying to look for ways to implement this.</p>
<h2 id="finding-the-perfect-implementation">Finding the perfect implementation</h2>
<h3 id="self-hosted-irc-server">Self-hosted IRC server</h3>
<p>Since the idea came from IRC, my first idea was to use the self-hosted server I used to try IRC in the first place. I had setup <a href="http://unrealircd.org/">UnrealIRCd</a> on my daily driver, and installed two IRC clients to try - <a href="https://www.codeux.com/textual/">Textual</a> (a GUI) and <a href="http://irssi.org/">irssi</a> (a TUI).</p>
<p>It all worked fine, but I came across its limitations pretty quickly. It doesn&rsquo;t support multi-line messages and making the chat persist across sessions required additional setup, which didn&rsquo;t seem worth the effort.</p>
<h3 id="self-hosted-zulip-server">Self-hosted Zulip server</h3>
<p>A great open-source alternative to Slack/Discord is <a href="https://zulipchat.com/">Zulip</a>, which is used as the primary chat platform at the <a href="https://www.recurse.com/scout/click?t=2ac66128b9116e0722d941ebf7e14b5a">Recurse Center</a>. I&rsquo;m familiar with the interface and it has the concept of channels, which I can use to manage projects.</p>
<p>After some trial and error, I was able to self-host an instance using their <a href="https://github.com/zulip/docker-zulip">docker image</a>. It worked well, but it also felt overkill for some reason. I realized I wanted something that didn&rsquo;t rely on the internet to function, but something that looked like a chat platform visually.</p>
<h3 id="custom-built-tui">Custom-built TUI</h3>
<p>I really like the <a href="https://github.com/charmbracelet/bubbletea">Bubbletea framework</a> for TUIs, and had it in my &ldquo;make something with this someday&rdquo; list. This seemed like a good chance, and I even found some projects as inspiration!</p>
<ul>
<li><a href="https://github.com/bashbunni/pjs">pjs</a> follows the same concept of time-stamped project logging, but with a different implementation. It uses a SQLite DB to store notes, and each note is displayed separately as opposed to a chat interface.</li>
<li><a href="https://github.com/maaslalani/nap">nap</a> is a code snippet manager that reads and displays plaintext files, and the layout is pretty close to what I had in mind.</li>
</ul>
<p>However, my feelings about TUIs are mixed - I find them really cool but don&rsquo;t use them much for some reason. This made me hesitant from really giving the idea a fair chance. I feared I would spend all this effort making something, and lose interest while making it.</p>
<p>This would have made for a great programming project though. Maybe someday.</p>
<h3 id="existing-gui-apps">Existing GUI apps</h3>
<p>I didn&rsquo;t expect to find much here, but I ended up finding multiple apps.</p>
<p>The one app I ended up trying was <a href="http://strflow.app/">Strflow</a>, a macOS app. It provides a chat-like interface, where you can type messages with a subset of Markdown and the option to add tags. Each tag then becomes its own view, so you either see all messages at once or filter by a specific tag. While I would have preferred separate chats for each project, filtering by tags seemed fair enough.</p>
<p>I tried Strflow for about a day, and it worked really well! It allowed me to import my existing notes (there&rsquo;s an option to select an earlier date) and had a floating window shortcut, which displays a tinier version of the chat interface. It didn&rsquo;t make me go &ldquo;this is it&rdquo; though.</p>
<p>Somewhere, I was still craving on making my own implementation, so I finally settled on an idea involving good &lsquo;ol plaintext files and Markdown.</p>
<h2 id="current-setup">Current setup</h2>
<p>I created a folder called <code>logs</code> which contains all of the project logs. Each project corresponds to one log file - <code>PROJNAME.md</code>, and follows this format:</p>
<pre><code>#### 27 Sep 2024

##### 16:54
Lorem ipsum dolor sit amet, consectetur adipiscing elit.

##### 20:55
Curabitur laoreet fermentum enim malesuada volutpat.

Praesent semper non odio at vestibulum.

...
</code></pre>
<p>I thought of choosing a shorter custom syntax, but if I ever want to export these logs as HTML, using existing syntax would make conversion and styling easier. I also don&rsquo;t use <code>h4</code> and <code>h5</code> tags in my writing, so using them here makes it easy to parse individual messages if I ever need to do that.</p>
<p>I added custom CSS set to my Markdown editor, <a href="https://typora.io/">Typora</a>, to render the log files in a cool way. The above log file renders like so:</p>
<blockquote class="log"><h4 id="27-sep-2024">27 Sep 2024</h4>
<h5 id="1654">16:54</h5>
<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p>
<h5 id="2055">20:55</h5>
<p>Curabitur laoreet fermentum enim malesuada volutpat.</p>
<p>Praesent semper non odio at vestibulum.</p>
<p>â€¦</p></blockquote>
<style>
.log h4,.log h5{font-size:95%;font-weight:normal;opacity:0.7;font-variant-numeric:tabular-nums}
.log h5{float:left;margin:0}
.log p{margin-left:4rem}
</style>
<p>Then there&rsquo;s the shell script to automate the logging process (parts of the script are omitted):</p>
<pre><code>file=&quot;$LOGDIR/$1.md&quot;
shift

if [[ -f &quot;$file&quot; ]]; then
        lastdate=&quot;$(grep '^#### ' &quot;$file&quot; | tail -n1 | cut -c5- )&quot;
        todate=&quot;$(date '+%b %d %Y')&quot;

        cat &lt;&lt;-EndOfMessage &gt;&gt; &quot;$file&quot;
                $(if [[ &quot;$lastdate&quot; != &quot;$todate&quot; ]]; then echo &quot;#### $todate&quot;; fi)
                ##### $(date '+%H:%M')
                ${@:-$($EDITOR $(mktemp))}
                EndOfMessage
fi
</code></pre>
<p>If the file exists, it extracts the last date in a file, and compares it with the current date. If the dates are different (for instance posting a note past midnight), it inserts the date, followed by the timestamp.</p>
<p>Line 11 is the content of the message. That cool bash syntax means &ldquo;take the remaining command line arguments if not empty, otherwise open a temporary file using an editor and take its contents after the editor is closed instead&rdquo;.</p>
<h2 id="next-steps">Next steps</h2>
<p>The interactivity of a chat interface is still missing - having a chat box that appears with a shortcut, or being able to post multiple messages in succession instead of calling the shell script everytime.</p>
<p>I&rsquo;m also starting to get interest in the TUI idea again, now that I recently learnt that my terminal emulator can <a href="https://iterm2.com/documentation-hotkey.html">display terminal windows with a shortcut</a>, and they can be <strong>floating windows</strong>.</p>
<p>I&rsquo;m also thinking of a web-based approach, with a frontend that resembles IRC/Slack.</p>
<p>Too many ideas in different directions, let&rsquo;s see where this goes.</p>
]]></description>
    </item>
    
    <item>
      <title>Installing Ubuntu on a 2017 MBP - Power management</title>
      <link>https://pjg1.site/mbp-linux-power.html</link>
      <pubDate>Fri, 13 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/mbp-linux-power.html</guid>
      <description><![CDATA[<p>Continuing from the <a href="/mbp-linux-wifi">previous post</a>, there were two other issues I faced after installing Ubuntu:</p>
<ul>
<li>The battery drained super quickly despite less usage</li>
<li>The laptop would get warm and remain warm even when the system was idle</li>
</ul>
<p>I didn&rsquo;t get any warnings regarding battery life or heat issues from the family member I got the laptop from, neither was I using the laptop enough for it to be this warm or use this much power.</p>
<p>Solving this wasn&rsquo;t as straightforward as the fixing WiFi, becuase the searches didn&rsquo;t lead to any one single solution. So I had to debug my way through this somehow.</p>
<h2 id="finding-a-starting-point">Finding a starting point</h2>
<p>I started by finding ways to see the temperature of my laptop, for which I found a package called <code>lm-sensors</code>. Before checking the temps, I ran <code>sensors-detect</code> and selected all of the default options.</p>
<pre><code>$ sudo apt install lm-sensors
$ sudo sensors-detect
</code></pre>
<p>When I ran <code>sensors</code> for the first time, there was too much output and it barely made sense. I spent some time deciphering the output, and then came the second problem - I didn&rsquo;t know the ideal temperatures to know which ones were high.</p>
<p>So I tried a different approach. I decided to capture the <code>sensors</code> output twice - once after boot, and one 30mins after that - and compare the two. In those 30mins, I tried to keep the system idle or used it minimally.</p>
<p>This approach worked, as I saw a difference in <code>coretemp-isa-0000</code>, which shows the temperatures of the CPU cores:</p>
<div class="flex">
<figure>
<pre><code>coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +45.0Â°C
...</code></pre>
<figcaption>After boot</figcaption>
</figure>
<figure>
<pre><code>coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +55.0Â°C
...</code></pre>
<figcaption>30mins after boot</figcaption>
</figure>
</div>
<p><code>Package id 0</code> refers to the temperature of the CPU as a whole, and there is a 10 degree increase in about 30mins, with little to no activity in that duration.</p>
<p>I was off to search again, and I landed with two possible causes of this:</p>
<ol>
<li>Some process is hogging CPU</li>
<li>Bad power management of Linux on Macs</li>
</ol>
<p>The first cause got eliminated pretty quickly, as <code>htop</code> didn&rsquo;t show any process with a high CPU usage, and the CPU usage was also fairly low overall. Bad power management was a very common issue reported in online forums, and I knew my machine worked fine on macOS, so this seemed like a valid cause.</p>
<p>One of the tools I came across to enable better power management was <code>powertop</code>, that displays the energy usage of a system and offers default settings for better power management. I enabled the defaults using the <code>--auto-tune</code> flag after installing.</p>
<pre><code>$ sudo apt install powertop
$ sudo powertop --auto-tune
</code></pre>
<p>When <code>powertop</code> is run without any flags, it runs in a similar fashion to <code>top</code>, displaying the energy usage and other statistics that update in real time.</p>
<pre><code>The battery reports a discharge rate of:  14.5  W
The energy consumed was :  325  J
The estimated remaining time is 2 hours, 6 minutes

Summary: 123.1 wakeups/second,  0.0 GPU ops/seconds, 0.0 VFS ops/sec and 3.1% CPU use

            Usage       Events/s    Category       Description
        675.2 Âµs/s      46.6        Timer          tick_sched_timer
          0.8 ms/s      21.0        Interrupt      [79] amdgpu
...
</code></pre>
<p>Some things stood out here:</p>
<ul>
<li>The battery discharge rate seemed high</li>
<li>As a result, the energy consumption was also high</li>
<li><code>amdgpu</code> was second highest in the energy usage list</li>
</ul>
<p>The appearance of <code>amdgpu</code> seemed something to look into further, and saw that there was an option to disable it all together. I wasn&rsquo;t planning on doing any heavy-duty work on this machine, so it seemed like a reasonable solution if it would help reduce temperatures.</p>
<p>I started following <a href="https://medium.com/codeflu/disabling-discrete-amd-graphics-card-in-linux-5d365738fc97">this tutorial</a>, which first checks if you have two graphics on your system or not.</p>
<pre><code>$ lspci | grep VGA
01:00.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Baffin [Radeon RX 460/560D / Pro 450/455/460/555/555X/560/560X] (rev ff)
</code></pre>
<p>I don&rsquo;t see a second GPU, but <a href="https://support.apple.com/en-us/111947">this machine</a> has two GPUs - an integrated GPU and a discrete/dedicated GPU.</p>
<p>OH WAIT, I found the root cause - the iGPU didn&rsquo;t get detected for whatever reason, and the dGPU is being used as the main graphics driver in its place. The dGPU uses a lot of power, which explains the the high energy usage in <code>powertop</code>, the quick battery drain and the laptop getting warm!</p>
<p>Other people have also <a href="https://github.com/Dunedan/mbp-2016-linux/issues/6">faced the same issue</a> and have documented solutions for it, which I followed along.</p>
<h2 id="enabling-the-igpu">Enabling the iGPU</h2>
<p>The iGPU is not detected thanks to the way Apple&rsquo;s firmware works. If it recognizes that it&rsquo;s booting an OS other than macOS, it will power down some of the hardware, the iGPU being one of them. Thanks Apple!</p>
<p>The TL;DR solution to this is to make the firmware believe that it is booting macOS by running custom code before boot.</p>
<h3 id="step-1-build-the-custom-efi-code">Step 1: Build the custom EFI code</h3>
<p>The custom code is available in the <a href="https://github.com/0xbb/apple_set_os.efi">apple_set_os.efi</a> repository. All I had to do was build the file.</p>
<pre><code>$ git clone https://github.com/0xbb/apple_set_os.efi
$ cd apple_set_os.efi
$ make
cc -I/usr/include/efi -I/usr/include/efi/x86_64 -DGNU_EFI_USE_MS_ABI -fPIC -fshort-wchar -ffreestanding -fno-stack-protector -maccumulate-outgoing-args -Wall -Dx86_64 -Werror -m64 -mno-red-zone   -c -o apple_set_os.o apple_set_os.c
ld -T /usr/lib/elf_x86_64_efi.lds -Bsymbolic -shared -nostdlib -znocombreloc /usr/lib/crt0-efi-x86_64.o -o apple_set_os.so apple_set_os.o /usr/lib/gcc/x86_64-linux-gnu/11/libgcc.a \
/usr/lib/libgnuefi.a
objcopy -j .text -j .sdata -j .data -j .dynamic -j .dynsym -j .rel \
        -j .rela -j .reloc -S --target=efi-app-x86_64 apple_set_os.so apple_set_os.efi
rm apple_set_os.o apple_set_os.so
</code></pre>
<h3 id="step-2-move-the-code-to-the-boot-partition">Step 2: move the code to the boot partition</h3>
<p>Next, the code needs to be in a location that is accessible during boot, aka the boot partition. I can put the code in <code>/boot/efi/EFI</code> directly too, but the instructions I was following put this in a sub-directory called <code>custom</code> instead.</p>
<pre><code>$ sudo mkdir /boot/efi/EFI/custom
$ sudo cp apple_set_os.efi /boot/efi/EFI/custom
</code></pre>
<h3 id="step-3-ask-grub-to-run-the-code-before-boot">Step 3: Ask GRUB to run the code before boot</h3>
<p>Placing the code in the boot partition alone isn&rsquo;t enough, I needed to add instructions to run the code before boot somewhere. That somewhere is the bootloader configuration, which in this case is GRUB. I added the following lines to a file created for users to add custom configurations: <code>/etc/grub.d/40_custom</code>:</p>
<pre><code>$ cat &lt;&lt;EOF &gt;&gt; /etc/grub.d/40_custom
search --no-floppy --set=root --file /EFI/custom/apple_set_os.efi
chainloader /EFI/custom/apple_set_os.efi
boot
EOF
</code></pre>
<p>The GRUB menu display was disabled on my machine. To be able to debug any issues on boot, I made the following changes to <code>/etc/default/grub</code>:</p>
<pre><code># Comment the following line
# GRUB_TIMEOUT_STYLE=hidden

# Change the timeout value
GRUB_TIMEOUT=10

# Uncomment the following lines
GRUB_TERMINAL=console
GRUB_GFXMODE=640x480
</code></pre>
<p>Then I ran <code>sudo update-grub</code> to save the changes.</p>
<h3 id="step-4-switch-to-using-the-igpu-on-boot">Step 4: Switch to using the iGPU on boot</h3>
<p>This is done using a shell script called <a href="https://github.com/0xbb/gpu-switch">gpu-switch</a> that writes the required values to an EFI variable to use the iGPU. The changes were applied on the next boot, so I rebooted the machine.</p>
<pre><code>$ git clone https://github.com/0xbb/gpu-switch
$ cd gpu-switch
$ sudo ./gpu-switch -i
$ sudo reboot now
</code></pre>
<p>After rebooting, the iGPU now appears in the <code>lspci</code> output!</p>
<pre><code>$ lspci | grep VGA
00:02.0 VGA compatible controller: Intel Corporation HD Graphics 630 (rev 04)
01:00.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Baffin [Radeon RX 460/560D / Pro 450/455/460/555/555X/560/560X] (rev ff)
</code></pre>
<h2 id="disable-dgpu">Disable dGPU</h2>
<p>The dGPU continued to run and warm up the laptop despite the iGPU being detected, so I disabled it with the following commands:</p>
<pre><code>$ echo OFF | sudo tee /sys/kernel/debug/vgaswitcheroo/switch
$ sudo modprobe -r amdgpu
</code></pre>
<p>And slowly, my laptop started to cool down. I checked the output of <code>sensors</code> after a while, and the temperatures were MUCH lower than with the dGPU enabled:</p>
<pre><code>coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +42.0Â°C
...
</code></pre>
<p>The <code>powertop</code> output also reflected this:</p>
<pre><code>The battery reports a discharge rate of 7.60 W
The energy consumed was 151 J
The estimated remaining time is 8 hours, 35 minutes

Summary: 62.1 wakeups/second,  0.0 GPU ops/seconds, 0.0 VFS ops/sec and 0.7% CPU use

            Usage       Events/s    Category       Description
        100.0%                      Device         Audio codec hwC1D0: ATI
        491.8 Âµs/s      27.3        Timer          tick_sched_timer
...
</code></pre>
<p>The battery discharge rate and energy consumption values were lower, battery life became longer and <code>amgdpu</code> no longer appeared at the top of the list!</p>
<p>Lastly, I created a systemd service to disable the dGPU on boot. Thanks to this, my machine remains cool throughout:</p>
<pre><code># disable-dgpu.service
[Unit]
Description=Disable discrete GPU
Before=display-manager.service

[Service]
Type=oneshot
ExecStart=/usr/sbin/modprobe amdgpu
ExecStart=/bin/sh -c 'echo OFF &gt; /sys/kernel/debug/vgaswitcheroo/switch'
ExecStart=/usr/sbin/modprobe -r amdgpu
RemainAfterExit=yes
TimeoutSec=0

[Install]
WantedBy=multi-user.target
</code></pre>
<p>I remember being scared when I noticed these issues for the first time. I&rsquo;d been used to things &ldquo;just working&rdquo; on macOS and Windows, and this was the opposite of that. Going from a feeling of fear to slowly gaining the courage to fix stuff has felt great. I think I&rsquo;m less scared now.</p>
]]></description>
    </item>
    
    <item>
      <title>Installing Ubuntu on a 2017 MBP - fixing WiFi troubles</title>
      <link>https://pjg1.site/mbp-linux-wifi.html</link>
      <pubDate>Mon, 09 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/mbp-linux-wifi.html</guid>
      <description><![CDATA[<p>I&rsquo;d been curious on what it&rsquo;s like to use Linux as the main OS, and wanted to upgrade from my <a href="/linux-vm-setup">Linux VM setup</a> for a while now. So when a family member was upgrading to a newer machine, I asked for their older machine - a <a href="https://support.apple.com/en-us/111947">2017 MacBook Pro</a> - and decided to install Linux on it.</p>
<p>I chose Ubuntu as that&rsquo;s the distro I&rsquo;m most comfortable with. After some initial research, I created a bootable installer using <a href="https://ubuntu.com/tutorials/create-a-usb-stick-on-macos#1-overview">this guide</a> and proceeded with the install which went smoothly. Ubuntu also booted much faster compared to macOS on the same machine, so I was feeling good about this.</p>
<p>The feel-good-ness went away fairly quickly, as I started to notice major problems - one of them being that the WiFi wouldn&rsquo;t connect. I could see the list of networks, but it got stuck at the connecting stage after I entered the password. I failed to catch this during the installation process as I skipped the &ldquo;Connect to WiFi&rdquo; step.</p>
<p>With the power of the search engine, I came to the conclusion that the WiFi driver was missing. The driver was available via <code>apt</code>, but I needed WiFi for <code>apt</code> to work. The solution I kept coming across was to get a direct connection via Ethernet and then install. Great idea, except my machine doesn&rsquo;t have an Ethernet port.</p>
<p><em>womp womp</em></p>
<p>While I was sure I would need an Ethernet adapter and even planned on buying one, I wrote about this situation in <a href="/weeknotes-32-2024">a weeknote</a> in the slight hopes of finding an alternate solution. And I DID! A fellow Recurser reached out with links I hadn&rsquo;t come across, and <a href="https://www.amirootyet.com/post/how-to-get-wifi-to-work-after#without-internet-connection">one of them</a> provided the required driver as a ZIP file with installation instructions.</p>
<p>And that&rsquo;s when it clicked - I can download the required files on a machine that has WiFi and transfer them via USB. A very obvious idea in hindsight, I&rsquo;m still wondering why I didn&rsquo;t think of this earlier.</p>
<p>While the file in the post didn&rsquo;t work, I now knew what to search for, and then came across <a href="https://askubuntu.com/questions/730799/installing-firmware-b43-installer-offline/730813#730813">this answer on askubuntu</a>. I downloaded the two files it suggests:</p>
<ul>
<li>the driver itself - <code>broadcom-wl-5.100.138</code></li>
<li>a package called <code>b43-fwcutter</code> that extracts the firmware specific to the hardware on my system</li>
</ul>
<p>Once I transferred these files via USB, I ran the remaining steps, and had working WiFi!!!</p>
<pre><code>$ sudo dpkg -i b43-fwcutter_015-9_amd64.deb
$ tar xfvj broadcom-wl-5.100.138.tar.bz2
$ sudo b43-fwcutter -w /lib/firmware broadcom-wl-5.100.138/linux/wl_apsta.o
$ sudo modprobe b43
</code></pre>
<p><a href="https://askubuntu.com/questions/626642/how-to-install-broadcom-wireless-drivers-offline/1244412#1244412">Another answer</a> in another thread suggested installing the package via <code>apt</code> once the WiFi works so future updates would be managed by <code>apt</code>, which I thought was a nice idea.</p>
<pre><code>$ sudo apt install b43-firmware-installer
</code></pre>
<p>The WiFi signal appeared to be pretty weak in the top bar, however I didn&rsquo;t face any issues while running <code>apt install</code>, interesting.</p>
<p>So problem solved, right? Not reaally. When I logged into the machine a few days later, the WiFi stopped connecting once again.</p>
<p>On further searching, I came across <a href="https://github.com/Dunedan/mbp-2016-linux">mbp-2016-linux</a> - an absolute gem of a resource that mentions what works and doesn&rsquo;t work on Linux for the 2016 and 2017 MacBook Pro&rsquo;s. Particularly, the section about WiFi caught my eye:</p>
<blockquote>
<p>The MacBook Pro models with Touch Bar come with a <code>Broadcom Limited BCM43602 802.11ac Wireless LAN SoC (rev 02)</code> which is also supported by <code>brcmfmac</code>, but has several issues rendering it unusable, caused by the available firmware.</p></blockquote>
<p>My machine has a Touch Bar, let me check what hardware I have:</p>
<pre><code>$ lspci -nn | grep Network
03:00.0 Network controller [0280]: Broadcom Inc. and subsidiaries BCM43602 802.11ac Wireless Lan SoC [14e4:43ba] (rev 02)
</code></pre>
<p>Aaand it&rsquo;s the same one. This was confusing since I got WiFi working earlier, that wasn&rsquo;t a dream. So now I needed to figure out what apart from installing the drivers made it work.</p>
<p>Then I remembered a command I ran in my initial attempt at fixing WiFi - copy-pasting commands without knowing what they do. It seemed relevant enough here for some reason, so I ran it again.</p>
<pre><code>$ sudo iwconfig wlp3s0 txpower 10dBm
</code></pre>
<p>Reconnected the WiFi, and I had a working connection again! I tried figuring out what led to this confusion, and here&rsquo;s what happened:</p>
<ul>
<li>The drivers were still missing when I ran the command the first time, so it didn&rsquo;t work on its own.</li>
<li>Once I had the correct driver, I forgot I ran this command and thought that the drivers alone made it work.</li>
</ul>
<p>This command is suggested as a workaround after installing the driver, and comes from this <a href="https://bugzilla.kernel.org/show_bug.cgi?id=193121">bug report</a>. I&rsquo;m not really sure why this works (if you do, let me know!), but it does explain why the WiFi signal appears weak after connecting.</p>
<p>Based on replies in the report and issues on mbp-2016-linux, this isn&rsquo;t a guaranteed fix but rather a &ldquo;your mileage may vary&rdquo; fix. I been using WiFi with this fix for a few weeks and haven&rsquo;t faced any issues, so looks like it seemed to work fine in my case (yay!)</p>
<p>Okay one last thing before I wrap up this post. This workaround isn&rsquo;t a one-time fix, so I had to type the command and restart the WiFi on each boot to make it work. That got repetitive, so I recently created and enabled a <code>systemd</code> service to run these commands on boot.</p>
<pre><code># mbp-linux-wifi.service
[Unit]
Description=Fixes to make WiFi work on a MBP running Linux
Requires=network.target
After=network.target

[Service]
Type=oneshot
ExecStart=/usr/sbin/iwconfig wlp3s0 txpower 10dBm
ExecStart=/usr/bin/systemctl restart NetworkManager
RemainAfterExit=yes
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
</code></pre>
<p>Earlier iterations of this service failed to run, since it would sometimes try to execute commands before the wireless interface (<code>wlp3s0</code>) is even detected. To solve this, I added the <code>Restart</code> and <code>RestartSec</code> parameters to retry after it fails, and now it connects shortly after booting.</p>
]]></description>
    </item>
    
    <item>
      <title>W35 2024: Taking a break from weeknotes</title>
      <link>https://pjg1.site/weeknotes-35-2024.html</link>
      <pubDate>Mon, 02 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://pjg1.site/weeknotes-35-2024.html</guid>
      <description><![CDATA[<h2 id="wifi-works-on-the-linux-machine">WiFi works on the Linux machine!</h2>
<p>I figured out why the WiFi stopped working again, and went down quite an interesting rabbit hole. I&rsquo;ve started working on a draft describing the situation (my first non-weeknote post after ages, eeeee), so I&rsquo;ll save details for that post.</p>
<p>One fun thing I did after WiFi started working was running an nginx server on my machine and accessing the default page from my daily driver. I&rsquo;ve tried this on a server in the cloud before, but running it on machine and accessing it from another machine in the same network felt very cool.</p>
<p>This homelab stuff makes me feel like a kid discovering things for the first time, and it feels awesome.</p>
<h2 id="the-website-fixes-continue">The website fixes continue</h2>
<p>After having pushed the initial version of the design, I still had some TODOs left, and even some feedback regarding the colors that I wanted to implement. Since the WiFi problem got solved quickly, I ended up spending the rest of my time on this. Some changes I made:</p>
<ul>
<li>Changed CSS - increased contrast for dark mode and made the colors more consistent across modes.</li>
<li>Added an introduction line to the homepage</li>
<li>Replaced terminal GIFs with <a href="https://asciinema.org">asciinema</a> recordings in posts</li>
<li>Moved <a href="/hanukkahofdata">Hanukkah of Data</a> from a standalone page to a post</li>
<li>Minor post edits</li>
<li>Added alt text to images</li>
<li>Realized that some images were probably not required while writing alt text, so deleted them</li>
<li>Added a <a href="/colophon">colophon</a> page</li>
</ul>
<h2 id="terminal-and-text-editor-themes-updated">Terminal and text editor themes updated</h2>
<p>Inspired by the minimal design of my website, I updated the themes for my text editor (Sublime Text atm) and Terminal application (iTerm) to match. On Sublime, I&rsquo;m using <a href="https://github.com/cowwabanga/sublime-almost-mono/tree/main">sublime-almost-mono</a> while I went for a completely monochrome theme for iTerm. Only white text on a black background (or vice-versa for light mode), no colors.</p>
<h2 id="changing-accountability-mechanisms">Changing accountability mechanisms</h2>
<p>I&rsquo;ve somehow managed to post weeknotes for about three months now - sometimes weekly, sometimes bi-weekly. I think it&rsquo;s time to take a break from them though, as it&rsquo;s no longer an effective accountability mechanism for me.</p>
<p>I hoped that writing these would help me get more comfortable publishing on a regular basis, and that would translate into me writing more technical posts focused on a particular topic. While I did get comfortable publishing, I haven&rsquo;t written any technical posts since I started these.</p>
<p>Now that I&rsquo;ve been able to generate ideas through other ways (identifying gaps in my knowledge and thinking of resources/projects to learn them), I would rather spend time working on those ideas and writing about them instead of weeknotes. It seems like a natural progression.</p>
<p>Weeknotes are a great format though, and I&rsquo;m glad I wrote them for as long as I did. I would encourage them to anyone who wants to write but is unsure what to write about, I&rsquo;m likely going to restart these whenever I&rsquo;m in a rut.</p>
<p>My new accountability mechanism is something more direct - an accountability buddy! <a href="https://swagnik.netlify.app">Swagnik</a> - a fellow Recurser - and I have started weekly meetings to reflect and discuss our plans for the week. The fact that I&rsquo;m being held accountable by one single person instead of putting out a message on a chat platform or this blog is making a difference, and probably what I needed.</p>
]]></description>
    </item>
    
  </channel>
</rss>
